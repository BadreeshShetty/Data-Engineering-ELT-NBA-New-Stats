[2024-06-18T16:33:22.276+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T16:33:22.378+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T16:33:22.399+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T16:33:22.403+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T16:33:22.431+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T16:33:22.461+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=1752) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T16:33:22.459+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmpx9s7sqwq']
[2024-06-18T16:33:22.466+0000] {standard_task_runner.py:91} INFO - Job 81: Subtask fetch_news
[2024-06-18T16:33:22.466+0000] {standard_task_runner.py:63} INFO - Started process 1835 to run task
[2024-06-18T16:33:22.641+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T16:33:22.887+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T16:33:22.892+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T16:33:23.298+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T16:33:23.298+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T16:33:23.355+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T163322, end_date=20240618T163323
[2024-06-18T16:33:23.418+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T16:33:23.447+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/models/baseoperator.py:1297 AirflowProviderDeprecationWarning: Call to deprecated class SnowflakeOperator. (This class is deprecated. Please use `***.providers.common.sql.operators.sql.SQLExecuteQueryOperator`. Also, you can provide `hook_params={'warehouse': <warehouse>, 'database': <database>, 'role': <role>, 'schema': <schema>, 'authenticator': <authenticator>,'session_parameters': <session_parameters>}`.)
[2024-06-18T16:33:23.476+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-18T16:33:23.484+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T16:42:18.654+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T16:42:18.719+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T16:42:18.728+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T16:42:18.728+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T16:42:18.745+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T16:42:18.767+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=7794) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T16:42:18.770+0000] {standard_task_runner.py:63} INFO - Started process 7876 to run task
[2024-06-18T16:42:18.769+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '101', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmprnqaojm6']
[2024-06-18T16:42:18.775+0000] {standard_task_runner.py:91} INFO - Job 101: Subtask fetch_news
[2024-06-18T16:42:18.997+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T16:42:19.189+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T16:42:19.191+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T16:42:19.505+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T16:42:19.505+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T16:42:19.560+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T164218, end_date=20240618T164219
[2024-06-18T16:42:19.595+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T16:42:19.622+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/models/baseoperator.py:1297 AirflowProviderDeprecationWarning: Call to deprecated class SnowflakeOperator. (This class is deprecated. Please use `***.providers.common.sql.operators.sql.SQLExecuteQueryOperator`. Also, you can provide `hook_params={'warehouse': <warehouse>, 'database': <database>, 'role': <role>, 'schema': <schema>, 'authenticator': <authenticator>,'session_parameters': <session_parameters>}`.)
[2024-06-18T16:42:19.643+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T16:42:19.645+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T16:49:03.284+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T16:49:03.356+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T16:49:03.372+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T16:49:03.372+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T16:49:03.396+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T16:49:03.424+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '115', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmp7a747qyl']
[2024-06-18T16:49:03.434+0000] {standard_task_runner.py:91} INFO - Job 115: Subtask fetch_news
[2024-06-18T16:49:03.433+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=11679) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T16:49:03.435+0000] {standard_task_runner.py:63} INFO - Started process 11765 to run task
[2024-06-18T16:49:03.619+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T16:49:03.799+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T16:49:03.800+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T16:49:04.006+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T16:49:04.006+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T16:49:04.085+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T164903, end_date=20240618T164904
[2024-06-18T16:49:04.139+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T16:49:04.196+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/models/baseoperator.py:1297 AirflowProviderDeprecationWarning: Call to deprecated class SnowflakeOperator. (This class is deprecated. Please use `***.providers.common.sql.operators.sql.SQLExecuteQueryOperator`. Also, you can provide `hook_params={'warehouse': <warehouse>, 'database': <database>, 'role': <role>, 'schema': <schema>, 'authenticator': <authenticator>,'session_parameters': <session_parameters>}`.)
[2024-06-18T16:49:04.229+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-18T16:49:04.233+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T17:37:18.208+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T17:37:18.291+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T17:37:18.301+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T17:37:18.301+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T17:37:18.318+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T17:37:18.337+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=2779) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T17:37:18.342+0000] {standard_task_runner.py:63} INFO - Started process 2875 to run task
[2024-06-18T17:37:18.339+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '141', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmpkipcrg5c']
[2024-06-18T17:37:18.346+0000] {standard_task_runner.py:91} INFO - Job 141: Subtask fetch_news
[2024-06-18T17:37:18.517+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T17:37:18.781+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T17:37:18.782+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T17:37:18.984+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T17:37:18.985+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T17:37:19.038+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T173718, end_date=20240618T173719
[2024-06-18T17:37:19.085+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T17:37:19.128+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T17:37:19.130+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T18:29:40.753+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T18:29:40.855+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T18:29:40.888+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T18:29:40.889+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T18:29:40.912+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T18:29:40.931+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '173', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmpzhnbwmg9']
[2024-06-18T18:29:40.937+0000] {standard_task_runner.py:91} INFO - Job 173: Subtask fetch_news
[2024-06-18T18:29:40.937+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=13530) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T18:29:40.938+0000] {standard_task_runner.py:63} INFO - Started process 13664 to run task
[2024-06-18T18:29:41.066+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T18:29:41.303+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T18:29:41.304+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T18:29:41.529+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T18:29:41.529+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T18:29:41.581+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T182940, end_date=20240618T182941
[2024-06-18T18:29:41.606+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T18:29:41.652+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T18:29:41.654+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T18:40:35.152+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T18:40:35.234+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T18:40:35.246+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T18:40:35.247+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T18:40:35.266+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T18:40:35.283+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=17695) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T18:40:35.286+0000] {standard_task_runner.py:63} INFO - Started process 17775 to run task
[2024-06-18T18:40:35.281+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '202', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmpu195in4e']
[2024-06-18T18:40:35.287+0000] {standard_task_runner.py:91} INFO - Job 202: Subtask fetch_news
[2024-06-18T18:40:35.410+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T18:40:35.644+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T18:40:35.645+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T18:40:35.916+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T18:40:35.917+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T18:40:35.969+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T184035, end_date=20240618T184035
[2024-06-18T18:40:36.029+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T18:40:36.105+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-18T18:40:36.108+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T18:53:20.565+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T18:53:20.642+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T18:53:20.654+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T18:53:20.654+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T18:53:20.676+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T18:53:20.692+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=21515) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T18:53:20.698+0000] {standard_task_runner.py:63} INFO - Started process 21592 to run task
[2024-06-18T18:53:20.696+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '219', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmp1eefcj91']
[2024-06-18T18:53:20.702+0000] {standard_task_runner.py:91} INFO - Job 219: Subtask fetch_news
[2024-06-18T18:53:20.889+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T18:53:21.090+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T18:53:21.092+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T18:53:21.403+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T18:53:21.404+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T18:53:21.456+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T185320, end_date=20240618T185321
[2024-06-18T18:53:21.484+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T18:53:21.531+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T18:53:21.533+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T19:08:46.991+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T19:08:47.058+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T19:08:47.065+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T19:08:47.066+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T19:08:47.085+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T19:08:47.104+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '240', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmp5b6nid8d']
[2024-06-18T19:08:47.113+0000] {standard_task_runner.py:91} INFO - Job 240: Subtask fetch_news
[2024-06-18T19:08:47.112+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=28018) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T19:08:47.114+0000] {standard_task_runner.py:63} INFO - Started process 28047 to run task
[2024-06-18T19:08:47.287+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T19:08:47.509+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T19:08:47.510+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T19:08:47.742+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T19:08:47.742+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T19:08:47.796+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T190847, end_date=20240618T190847
[2024-06-18T19:08:47.823+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T19:08:47.868+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T19:08:47.873+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T19:18:26.529+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T19:18:26.576+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T19:18:26.586+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T19:18:26.586+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T19:18:26.612+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T19:18:26.634+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '259', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmpgbzw0dzo']
[2024-06-18T19:18:26.642+0000] {standard_task_runner.py:91} INFO - Job 259: Subtask fetch_news
[2024-06-18T19:18:26.643+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=32003) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T19:18:26.644+0000] {standard_task_runner.py:63} INFO - Started process 32106 to run task
[2024-06-18T19:18:26.782+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T19:18:26.994+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T19:18:26.995+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T19:18:27.353+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T19:18:27.353+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T19:18:27.418+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T191826, end_date=20240618T191827
[2024-06-18T19:18:27.470+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T19:18:27.522+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-18T19:18:27.528+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T20:33:09.391+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T20:33:09.493+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T20:33:09.516+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T20:33:09.521+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T20:33:09.552+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): fetch_news> on 2024-06-17 00:00:00+00:00
[2024-06-18T20:33:09.588+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline', 'fetch_news', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '272', '--raw', '--subdir', 'DAGS_FOLDER/nba_news_stats_etl.py', '--cfg-path', '/tmp/tmpkg_ulqm1']
[2024-06-18T20:33:09.600+0000] {standard_task_runner.py:91} INFO - Job 272: Subtask fetch_news
[2024-06-18T20:33:09.601+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=2133) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T20:33:09.602+0000] {standard_task_runner.py:63} INFO - Started process 2213 to run task
[2024-06-18T20:33:09.844+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline.fetch_news scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T20:33:10.232+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='fetch_news' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T20:33:10.233+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T20:33:10.850+0000] {python.py:237} INFO - Done. Returned value was: /home/ubuntu/dags/nba_news_stats_data/nba_news.parquet
[2024-06-18T20:33:10.851+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T20:33:10.985+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=fetch_news, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T203309, end_date=20240618T203310
[2024-06-18T20:33:11.045+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T20:33:11.094+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T20:33:11.097+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
