[2024-06-20T11:34:12.582+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:34:12.721+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T11:34:12.745+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T11:34:12.746+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:34:12.781+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T11:34:12.845+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3616) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:34:12.822+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '810', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprvt3fslo']
[2024-06-20T11:34:12.847+0000] {standard_task_runner.py:91} INFO - Job 810: Subtask upload_scores_to_s3
[2024-06-20T11:34:12.846+0000] {standard_task_runner.py:63} INFO - Started process 3852 to run task
[2024-06-20T11:34:13.216+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:34:13.878+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T11:34:13.882+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:34:14.492+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:34:15.104+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T11:34:15.105+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:34:15.188+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T113412, end_date=20240620T113415
[2024-06-20T11:34:15.218+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:34:15.267+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:34:15.273+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:40:22.440+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:40:22.607+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T11:40:22.659+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T11:40:22.659+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:40:22.721+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T11:40:22.767+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8027) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:40:22.778+0000] {standard_task_runner.py:63} INFO - Started process 8222 to run task
[2024-06-20T11:40:22.766+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '839', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpi8lyf39i']
[2024-06-20T11:40:22.783+0000] {standard_task_runner.py:91} INFO - Job 839: Subtask upload_scores_to_s3
[2024-06-20T11:40:23.226+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:40:23.915+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T11:40:23.921+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:40:24.349+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:40:25.025+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T11:40:25.025+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:40:25.122+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T114022, end_date=20240620T114025
[2024-06-20T11:40:25.197+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:40:25.315+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:52:24.223+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:52:24.385+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T11:52:24.435+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T11:52:24.435+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:52:24.493+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T11:52:24.530+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '875', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpwxlutl1q']
[2024-06-20T11:52:24.544+0000] {standard_task_runner.py:91} INFO - Job 875: Subtask upload_scores_to_s3
[2024-06-20T11:52:24.547+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=14962) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:52:24.554+0000] {standard_task_runner.py:63} INFO - Started process 15132 to run task
[2024-06-20T11:52:24.911+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:52:25.576+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T11:52:25.581+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:52:26.029+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:52:26.602+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T11:52:26.603+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:52:26.755+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T115224, end_date=20240620T115226
[2024-06-20T11:52:26.847+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:52:26.971+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:52:26.977+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:12:20.827+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:12:21.034+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:12:21.085+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:12:21.085+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:12:21.142+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T12:12:21.192+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '924', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpp6m3u_9x']
[2024-06-20T12:12:21.199+0000] {standard_task_runner.py:91} INFO - Job 924: Subtask upload_scores_to_s3
[2024-06-20T12:12:21.202+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=22949) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:12:21.203+0000] {standard_task_runner.py:63} INFO - Started process 23119 to run task
[2024-06-20T12:12:21.522+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:12:22.060+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T12:12:22.069+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:12:22.609+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:12:23.090+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:12:23.093+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:12:23.253+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T121221, end_date=20240620T121223
[2024-06-20T12:12:23.310+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:12:23.462+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:12:23.465+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:20:45.532+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:20:45.659+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:20:45.685+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:20:45.686+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:20:45.720+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T12:20:45.736+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=29161) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:20:45.740+0000] {standard_task_runner.py:63} INFO - Started process 29301 to run task
[2024-06-20T12:20:45.738+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '958', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpl92o8t2g']
[2024-06-20T12:20:45.742+0000] {standard_task_runner.py:91} INFO - Job 958: Subtask upload_scores_to_s3
[2024-06-20T12:20:46.144+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:20:46.667+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T12:20:46.672+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:20:47.067+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:20:47.635+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:20:47.635+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:20:47.759+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T122045, end_date=20240620T122047
[2024-06-20T12:20:47.816+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:20:47.919+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:37:46.815+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:37:46.975+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:37:47.020+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:37:47.021+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:37:47.084+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T12:37:47.134+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=34642) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:37:47.142+0000] {standard_task_runner.py:63} INFO - Started process 34807 to run task
[2024-06-20T12:37:47.144+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '990', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpt5z08iil']
[2024-06-20T12:37:47.153+0000] {standard_task_runner.py:91} INFO - Job 990: Subtask upload_scores_to_s3
[2024-06-20T12:37:47.502+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:37:47.877+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T12:37:47.882+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:37:48.481+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:37:49.088+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:37:49.089+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:37:49.290+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T123746, end_date=20240620T123749
[2024-06-20T12:37:49.399+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:37:49.531+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:37:49.535+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:39:40.360+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:39:40.502+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:39:40.516+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:39:40.517+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:39:40.534+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T12:39:40.600+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=37281) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:39:40.585+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1012', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpn3qb2a27']
[2024-06-20T12:39:40.609+0000] {standard_task_runner.py:91} INFO - Job 1012: Subtask upload_scores_to_s3
[2024-06-20T12:39:40.609+0000] {standard_task_runner.py:63} INFO - Started process 37461 to run task
[2024-06-20T12:39:41.073+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:39:41.642+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T12:39:41.644+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:39:42.171+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:39:42.784+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:39:42.791+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:39:43.016+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T123940, end_date=20240620T123943
[2024-06-20T12:39:43.108+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:39:43.300+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:39:43.311+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:49:14.181+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:49:14.364+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:49:14.419+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:49:14.420+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:49:14.485+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T12:49:14.545+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=43106) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:49:14.558+0000] {standard_task_runner.py:63} INFO - Started process 43291 to run task
[2024-06-20T12:49:14.545+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1045', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp23aw5l0f']
[2024-06-20T12:49:14.565+0000] {standard_task_runner.py:91} INFO - Job 1045: Subtask upload_scores_to_s3
[2024-06-20T12:49:14.974+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:49:15.625+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T12:49:15.627+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:49:16.426+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:49:17.109+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:49:17.110+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:49:17.302+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T124914, end_date=20240620T124917
[2024-06-20T12:49:17.413+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:49:17.559+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:49:17.566+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:54:43.551+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:54:43.707+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:54:43.748+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T12:54:43.749+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:54:43.785+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T12:54:43.840+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1084', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpqn6aw124']
[2024-06-20T12:54:43.849+0000] {standard_task_runner.py:91} INFO - Job 1084: Subtask upload_scores_to_s3
[2024-06-20T12:54:43.853+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=49182) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:54:43.866+0000] {standard_task_runner.py:63} INFO - Started process 49441 to run task
[2024-06-20T12:54:44.070+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:54:44.459+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T12:54:44.461+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:54:45.129+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:54:45.673+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:54:45.680+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:54:45.844+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T125443, end_date=20240620T125445
[2024-06-20T12:54:45.913+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:54:46.041+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:54:46.045+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T13:06:25.685+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T13:06:25.856+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T13:06:25.899+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T13:06:25.900+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T13:06:25.959+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T13:06:26.007+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=55646) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T13:06:26.019+0000] {standard_task_runner.py:63} INFO - Started process 55734 to run task
[2024-06-20T13:06:26.013+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1119', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpp_3ng1pb']
[2024-06-20T13:06:26.024+0000] {standard_task_runner.py:91} INFO - Job 1119: Subtask upload_scores_to_s3
[2024-06-20T13:06:26.453+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T13:06:26.941+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T13:06:26.947+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T13:06:27.677+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T13:06:28.397+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T13:06:28.401+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T13:06:28.565+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T130625, end_date=20240620T130628
[2024-06-20T13:06:28.650+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T13:06:28.808+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T13:06:28.816+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:34:14.291+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:34:14.441+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T16:34:14.484+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T16:34:14.487+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:34:14.524+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T16:34:14.559+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1151', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpt8wglgjv']
[2024-06-20T16:34:14.576+0000] {standard_task_runner.py:91} INFO - Job 1151: Subtask upload_scores_to_s3
[2024-06-20T16:34:14.573+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4831) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:34:14.578+0000] {standard_task_runner.py:63} INFO - Started process 4982 to run task
[2024-06-20T16:34:14.882+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:34:15.523+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T16:34:15.524+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:34:16.051+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:34:16.504+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T16:34:16.504+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:34:16.605+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T163414, end_date=20240620T163416
[2024-06-20T16:34:16.749+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:34:16.904+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:34:16.912+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:43:44.429+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:43:44.613+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T16:43:44.664+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T16:43:44.664+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:43:44.711+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T16:43:44.748+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=10505) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:43:44.764+0000] {standard_task_runner.py:63} INFO - Started process 10675 to run task
[2024-06-20T16:43:44.752+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1190', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpow1atuwj']
[2024-06-20T16:43:44.768+0000] {standard_task_runner.py:91} INFO - Job 1190: Subtask upload_scores_to_s3
[2024-06-20T16:43:45.083+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:43:45.485+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T16:43:45.486+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:43:45.953+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:43:46.527+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T16:43:46.531+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:43:46.659+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T164344, end_date=20240620T164346
[2024-06-20T16:43:46.724+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:43:46.873+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:43:46.880+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:54:19.392+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:54:19.517+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T16:54:19.556+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T16:54:19.556+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:54:19.600+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T16:54:19.626+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=16254) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:54:19.630+0000] {standard_task_runner.py:63} INFO - Started process 16319 to run task
[2024-06-20T16:54:19.634+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1219', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmphuzlcpw_']
[2024-06-20T16:54:19.643+0000] {standard_task_runner.py:91} INFO - Job 1219: Subtask upload_scores_to_s3
[2024-06-20T16:54:20.014+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:54:20.461+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T16:54:20.462+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:54:20.911+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:54:21.392+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T16:54:21.397+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:54:21.566+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T165419, end_date=20240620T165421
[2024-06-20T16:54:21.626+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:54:21.734+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:54:21.738+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:00:38.729+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:00:38.799+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T17:00:38.832+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T17:00:38.832+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:00:38.871+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T17:00:38.894+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=21938) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:00:38.897+0000] {standard_task_runner.py:63} INFO - Started process 22124 to run task
[2024-06-20T17:00:38.893+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1252', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpn_4hubf_']
[2024-06-20T17:00:38.899+0000] {standard_task_runner.py:91} INFO - Job 1252: Subtask upload_scores_to_s3
[2024-06-20T17:00:39.072+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:00:39.385+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T17:00:39.386+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:00:39.931+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:00:40.378+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T17:00:40.378+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:00:40.565+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T170038, end_date=20240620T170040
[2024-06-20T17:00:40.627+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:00:40.693+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:40:54.014+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:40:54.160+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T17:40:54.186+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T17:40:54.186+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:40:54.235+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T17:40:54.270+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=28608) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:40:54.274+0000] {standard_task_runner.py:63} INFO - Started process 28774 to run task
[2024-06-20T17:40:54.267+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1282', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpajd6rxou']
[2024-06-20T17:40:54.275+0000] {standard_task_runner.py:91} INFO - Job 1282: Subtask upload_scores_to_s3
[2024-06-20T17:40:54.630+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:40:55.116+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T17:40:55.117+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:40:55.563+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:40:55.971+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T17:40:55.974+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:40:56.098+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T174054, end_date=20240620T174056
[2024-06-20T17:40:56.199+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:40:56.280+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:03:18.024+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:03:18.118+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:03:18.134+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:03:18.139+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:03:18.159+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:03:18.186+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=35977) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:03:18.192+0000] {standard_task_runner.py:63} INFO - Started process 36110 to run task
[2024-06-20T18:03:18.188+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1318', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp8ixf03s9']
[2024-06-20T18:03:18.199+0000] {standard_task_runner.py:91} INFO - Job 1318: Subtask upload_scores_to_s3
[2024-06-20T18:03:18.506+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:03:19.066+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:03:19.067+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:03:19.608+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:03:20.085+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:03:20.088+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:03:20.216+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T180318, end_date=20240620T180320
[2024-06-20T18:03:20.285+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:03:20.418+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:03:20.430+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:07:27.138+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:07:27.326+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:07:27.357+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:07:27.358+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:07:27.394+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:07:27.430+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=41027) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:07:27.431+0000] {standard_task_runner.py:63} INFO - Started process 41184 to run task
[2024-06-20T18:07:27.426+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1350', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpfw0l5owz']
[2024-06-20T18:07:27.434+0000] {standard_task_runner.py:91} INFO - Job 1350: Subtask upload_scores_to_s3
[2024-06-20T18:07:27.729+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:07:28.232+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:07:28.235+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:07:28.505+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:07:29.026+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:07:29.032+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:07:29.199+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T180727, end_date=20240620T180729
[2024-06-20T18:07:29.281+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:07:29.436+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:07:29.444+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:12:10.493+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:12:10.667+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:12:10.719+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:12:10.719+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:12:10.769+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:12:10.811+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=45972) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:12:10.812+0000] {standard_task_runner.py:63} INFO - Started process 46132 to run task
[2024-06-20T18:12:10.810+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1385', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmppeugcssc']
[2024-06-20T18:12:10.821+0000] {standard_task_runner.py:91} INFO - Job 1385: Subtask upload_scores_to_s3
[2024-06-20T18:12:11.150+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:12:11.581+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:12:11.585+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:12:11.997+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:12:12.424+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:12:12.425+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:12:12.511+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T181210, end_date=20240620T181212
[2024-06-20T18:12:12.581+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:12:12.638+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:16:48.117+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:16:48.283+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:16:48.298+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:16:48.302+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:16:48.340+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:16:48.398+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=50087) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:16:48.393+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1415', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp6aqs6xzk']
[2024-06-20T18:16:48.408+0000] {standard_task_runner.py:91} INFO - Job 1415: Subtask upload_scores_to_s3
[2024-06-20T18:16:48.407+0000] {standard_task_runner.py:63} INFO - Started process 50261 to run task
[2024-06-20T18:16:48.708+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:16:49.393+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:16:49.400+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:16:49.891+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:16:50.368+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:16:50.368+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:16:50.454+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T181648, end_date=20240620T181650
[2024-06-20T18:16:50.530+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:16:50.723+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:16:50.728+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:23:53.697+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:23:53.859+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:23:53.910+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:23:53.911+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:23:53.967+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:23:54.007+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=56761) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:23:54.019+0000] {standard_task_runner.py:63} INFO - Started process 56985 to run task
[2024-06-20T18:23:54.010+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1454', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp7g5p7zw6']
[2024-06-20T18:23:54.024+0000] {standard_task_runner.py:91} INFO - Job 1454: Subtask upload_scores_to_s3
[2024-06-20T18:23:54.260+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:23:54.804+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:23:54.807+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:23:55.337+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:23:55.859+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:23:55.859+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:23:56.087+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T182353, end_date=20240620T182356
[2024-06-20T18:23:56.346+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:23:56.481+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:23:56.489+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:27:21.162+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:27:21.261+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:27:21.272+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:27:21.272+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:27:21.300+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:27:21.342+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1490', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpn68yvs7p']
[2024-06-20T18:27:21.346+0000] {standard_task_runner.py:91} INFO - Job 1490: Subtask upload_scores_to_s3
[2024-06-20T18:27:21.341+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=61483) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:27:21.347+0000] {standard_task_runner.py:63} INFO - Started process 61720 to run task
[2024-06-20T18:27:21.620+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:27:22.218+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:27:22.221+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:27:22.784+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:27:23.185+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:27:23.196+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:27:23.449+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T182721, end_date=20240620T182723
[2024-06-20T18:27:23.588+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:27:23.713+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:30:39.799+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:30:39.934+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:30:39.995+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:30:39.995+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:30:40.105+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:30:40.145+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=65602) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:30:40.151+0000] {standard_task_runner.py:63} INFO - Started process 65778 to run task
[2024-06-20T18:30:40.147+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1519', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmptyiedy9n']
[2024-06-20T18:30:40.155+0000] {standard_task_runner.py:91} INFO - Job 1519: Subtask upload_scores_to_s3
[2024-06-20T18:30:40.551+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:30:41.042+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:30:41.043+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:30:41.577+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:30:42.118+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:30:42.118+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:30:42.261+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T183039, end_date=20240620T183042
[2024-06-20T18:30:42.318+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:30:42.394+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:30:42.396+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:37:53.299+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:37:53.477+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:37:53.540+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T18:37:53.540+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:37:53.611+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T18:37:53.645+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1555', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpmff6yku3']
[2024-06-20T18:37:53.657+0000] {standard_task_runner.py:91} INFO - Job 1555: Subtask upload_scores_to_s3
[2024-06-20T18:37:53.653+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=71357) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:37:53.661+0000] {standard_task_runner.py:63} INFO - Started process 71526 to run task
[2024-06-20T18:37:53.947+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:37:54.555+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T18:37:54.556+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:37:55.176+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:37:55.954+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:37:55.962+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:37:56.148+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T183753, end_date=20240620T183756
[2024-06-20T18:37:56.228+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:37:56.377+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:37:56.378+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:19:32.166+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:19:32.341+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T21:19:32.403+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T21:19:32.403+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:19:32.501+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T21:19:32.544+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1589', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpirz69uk3']
[2024-06-20T21:19:32.560+0000] {standard_task_runner.py:91} INFO - Job 1589: Subtask upload_scores_to_s3
[2024-06-20T21:19:32.557+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3943) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:19:32.564+0000] {standard_task_runner.py:63} INFO - Started process 4094 to run task
[2024-06-20T21:19:32.988+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:19:33.540+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T21:19:33.541+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:19:34.153+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:19:34.696+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T21:19:34.696+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:19:34.819+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T211932, end_date=20240620T211934
[2024-06-20T21:19:34.867+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:19:34.984+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:19:34.990+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:25:00.830+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:25:00.989+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T21:25:01.030+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T21:25:01.031+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:25:01.103+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T21:25:01.155+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8860) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:25:01.156+0000] {standard_task_runner.py:63} INFO - Started process 9018 to run task
[2024-06-20T21:25:01.157+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1627', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp0r5pt196']
[2024-06-20T21:25:01.164+0000] {standard_task_runner.py:91} INFO - Job 1627: Subtask upload_scores_to_s3
[2024-06-20T21:25:01.483+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:25:02.205+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T21:25:02.206+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:25:02.886+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:25:03.386+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T21:25:03.389+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:25:03.575+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T212500, end_date=20240620T212503
[2024-06-20T21:25:03.654+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:25:03.849+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:25:03.854+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T22:07:12.494+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T22:07:12.550+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T22:07:12.567+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [queued]>
[2024-06-20T22:07:12.568+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T22:07:12.620+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-19 00:00:00+00:00
[2024-06-20T22:07:12.654+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-19T00:00:00+00:00', '--job-id', '1678', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxqb5cu7u']
[2024-06-20T22:07:12.668+0000] {standard_task_runner.py:91} INFO - Job 1678: Subtask upload_scores_to_s3
[2024-06-20T22:07:12.674+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=7979) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T22:07:12.685+0000] {standard_task_runner.py:63} INFO - Started process 8152 to run task
[2024-06-20T22:07:13.033+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-19T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T22:07:13.530+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-19T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-19T00:00:00+00:00'
[2024-06-20T22:07:13.534+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T22:07:14.054+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T22:07:14.557+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T22:07:14.558+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T22:07:14.661+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-19T00:00:00+00:00, execution_date=20240619T000000, start_date=20240620T220712, end_date=20240620T220714
[2024-06-20T22:07:14.732+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T22:07:14.855+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T22:07:14.862+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
