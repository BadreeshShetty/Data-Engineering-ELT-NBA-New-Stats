[2024-06-19T00:01:37.288+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T00:01:37.329+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:01:37.337+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:01:37.337+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T00:01:37.350+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T00:01:37.359+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '569', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpa0v6xifm']
[2024-06-19T00:01:37.362+0000] {standard_task_runner.py:91} INFO - Job 569: Subtask upload_scores_to_s3
[2024-06-19T00:01:37.362+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=9364) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T00:01:37.363+0000] {standard_task_runner.py:63} INFO - Started process 9383 to run task
[2024-06-19T00:01:37.456+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T00:01:37.620+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T00:01:37.621+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T00:01:37.821+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T00:01:38.006+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T00:01:38.006+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T00:01:38.061+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T000137, end_date=20240619T000138
[2024-06-19T00:01:38.101+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T00:01:38.138+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T00:01:38.141+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T00:36:37.605+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T00:36:37.647+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:36:37.656+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:36:37.657+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T00:36:37.669+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T00:36:37.679+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '593', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpvwtf94wp']
[2024-06-19T00:36:37.682+0000] {standard_task_runner.py:91} INFO - Job 593: Subtask upload_scores_to_s3
[2024-06-19T00:36:37.682+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3148) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T00:36:37.683+0000] {standard_task_runner.py:63} INFO - Started process 3248 to run task
[2024-06-19T00:36:37.777+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T00:36:37.952+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T00:36:37.953+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T00:36:38.253+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T00:36:38.487+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T00:36:38.487+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T00:36:38.542+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T003637, end_date=20240619T003638
[2024-06-19T00:36:38.581+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T00:36:38.619+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T00:36:38.621+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T00:46:46.021+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T00:46:46.066+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:46:46.075+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:46:46.075+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T00:46:46.090+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T00:46:46.106+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=7989) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T00:46:46.108+0000] {standard_task_runner.py:63} INFO - Started process 8017 to run task
[2024-06-19T00:46:46.104+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '614', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxeutdk1_']
[2024-06-19T00:46:46.110+0000] {standard_task_runner.py:91} INFO - Job 614: Subtask upload_scores_to_s3
[2024-06-19T00:46:46.213+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T00:46:46.411+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T00:46:46.412+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T00:46:46.594+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T00:46:46.773+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T00:46:46.773+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T00:46:46.826+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T004646, end_date=20240619T004646
[2024-06-19T00:46:46.848+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T00:46:46.887+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T00:46:46.891+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T01:09:22.471+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T01:09:22.516+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:09:22.524+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:09:22.525+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T01:09:22.537+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T01:09:22.553+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=14484) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T01:09:22.554+0000] {standard_task_runner.py:63} INFO - Started process 14542 to run task
[2024-06-19T01:09:22.553+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '634', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpkixbg_qw']
[2024-06-19T01:09:22.556+0000] {standard_task_runner.py:91} INFO - Job 634: Subtask upload_scores_to_s3
[2024-06-19T01:09:22.719+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T01:09:22.888+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T01:09:22.891+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T01:09:23.032+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T01:09:23.217+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T01:09:23.221+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T01:09:23.317+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T010922, end_date=20240619T010923
[2024-06-19T01:09:23.336+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T01:09:23.375+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T01:09:23.377+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T01:40:16.853+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T01:40:16.898+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:40:16.909+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:40:16.909+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T01:40:16.923+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T01:40:16.938+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=22031) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T01:40:16.940+0000] {standard_task_runner.py:63} INFO - Started process 22063 to run task
[2024-06-19T01:40:16.940+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '657', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpm1iclj7k']
[2024-06-19T01:40:16.943+0000] {standard_task_runner.py:91} INFO - Job 657: Subtask upload_scores_to_s3
[2024-06-19T01:40:17.118+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T01:40:17.310+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T01:40:17.311+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T01:40:17.486+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T01:40:17.681+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T01:40:17.682+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T01:40:17.740+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T014016, end_date=20240619T014017
[2024-06-19T01:40:17.760+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T01:40:17.799+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T01:40:17.801+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T01:55:39.119+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T01:55:39.161+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:55:39.170+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:55:39.170+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T01:55:39.185+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T01:55:39.201+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=26959) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T01:55:39.202+0000] {standard_task_runner.py:63} INFO - Started process 26992 to run task
[2024-06-19T01:55:39.199+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '675', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmphcjk1hm9']
[2024-06-19T01:55:39.204+0000] {standard_task_runner.py:91} INFO - Job 675: Subtask upload_scores_to_s3
[2024-06-19T01:55:39.304+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T01:55:39.477+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T01:55:39.478+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T01:55:39.650+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T01:55:39.828+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T01:55:39.829+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T01:55:39.882+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T015539, end_date=20240619T015539
[2024-06-19T01:55:39.903+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T01:55:39.940+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T01:55:39.942+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T04:28:51.495+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T04:28:51.539+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T04:28:51.548+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T04:28:51.549+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T04:28:51.562+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T04:28:51.571+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '691', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp3jpg7j0g']
[2024-06-19T04:28:51.574+0000] {standard_task_runner.py:91} INFO - Job 691: Subtask upload_scores_to_s3
[2024-06-19T04:28:51.574+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3431) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T04:28:51.575+0000] {standard_task_runner.py:63} INFO - Started process 3530 to run task
[2024-06-19T04:28:51.668+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T04:28:51.840+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T04:28:51.843+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T04:28:52.138+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T04:28:52.370+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T04:28:52.371+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T04:28:52.426+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T042851, end_date=20240619T042852
[2024-06-19T04:28:52.473+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T04:28:52.512+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T04:28:52.514+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:53:59.839+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:53:59.880+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:53:59.888+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:53:59.889+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T13:53:59.901+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:53:59.915+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '710', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpv9zkpqnv']
[2024-06-19T13:53:59.919+0000] {standard_task_runner.py:91} INFO - Job 710: Subtask upload_scores_to_s3
[2024-06-19T13:53:59.923+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=5339) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T13:53:59.925+0000] {standard_task_runner.py:63} INFO - Started process 5370 to run task
[2024-06-19T13:54:00.027+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T13:54:00.215+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:54:00.217+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:54:00.488+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T13:54:00.716+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T13:54:00.716+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:54:00.770+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T135359, end_date=20240619T135400
[2024-06-19T13:54:00.828+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:54:00.863+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:54:00.865+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T17:10:31.173+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T17:10:31.253+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:10:31.285+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:10:31.286+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T17:10:31.323+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T17:10:31.351+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '729', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpsh4dp9yv']
[2024-06-19T17:10:31.374+0000] {standard_task_runner.py:91} INFO - Job 729: Subtask upload_scores_to_s3
[2024-06-19T17:10:31.376+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4164) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T17:10:31.377+0000] {standard_task_runner.py:63} INFO - Started process 4206 to run task
[2024-06-19T17:10:31.651+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T17:10:32.011+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T17:10:32.017+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T17:10:32.445+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T17:10:32.810+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T17:10:32.810+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T17:10:32.866+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T171031, end_date=20240619T171032
[2024-06-19T17:10:32.897+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T17:10:32.947+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T17:10:32.950+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T17:25:41.396+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T17:25:41.474+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:25:41.501+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:25:41.501+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T17:25:41.526+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T17:25:41.546+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8691) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T17:25:41.547+0000] {standard_task_runner.py:63} INFO - Started process 8720 to run task
[2024-06-19T17:25:41.542+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '751', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpu2moz9b1']
[2024-06-19T17:25:41.549+0000] {standard_task_runner.py:91} INFO - Job 751: Subtask upload_scores_to_s3
[2024-06-19T17:25:41.734+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T17:25:41.956+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T17:25:41.961+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T17:25:42.117+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T17:25:42.368+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T17:25:42.368+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T17:25:42.439+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T172541, end_date=20240619T172542
[2024-06-19T17:25:42.499+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T17:25:42.553+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T17:25:42.556+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T21:12:31.755+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T21:12:31.847+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T21:12:31.875+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T21:12:31.876+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T21:12:31.895+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T21:12:31.913+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '771', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmplotpccnc']
[2024-06-19T21:12:31.919+0000] {standard_task_runner.py:91} INFO - Job 771: Subtask upload_scores_to_s3
[2024-06-19T21:12:31.921+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4568) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T21:12:31.924+0000] {standard_task_runner.py:63} INFO - Started process 4587 to run task
[2024-06-19T21:12:32.072+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T21:12:32.371+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T21:12:32.372+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T21:12:32.725+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T21:12:32.981+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-19T21:12:32.982+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T21:12:33.040+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T211231, end_date=20240619T211233
[2024-06-19T21:12:33.075+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T21:12:33.135+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T21:12:33.150+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:34:11.349+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:34:11.544+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:34:11.599+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:34:11.599+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:34:11.652+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:34:11.715+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '805', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpfs1qqdjk']
[2024-06-20T11:34:11.741+0000] {standard_task_runner.py:91} INFO - Job 805: Subtask upload_scores_to_s3
[2024-06-20T11:34:11.743+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3570) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:34:11.744+0000] {standard_task_runner.py:63} INFO - Started process 3813 to run task
[2024-06-20T11:34:12.195+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:34:12.949+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:34:12.954+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:34:13.787+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:34:14.412+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T11:34:14.418+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:34:14.590+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T113411, end_date=20240620T113414
[2024-06-20T11:34:14.691+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:34:14.785+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:40:22.444+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:40:22.608+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:40:22.652+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:40:22.654+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:40:22.718+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:40:22.767+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8026) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:40:22.779+0000] {standard_task_runner.py:63} INFO - Started process 8221 to run task
[2024-06-20T11:40:22.761+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '838', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprh74aohq']
[2024-06-20T11:40:22.787+0000] {standard_task_runner.py:91} INFO - Job 838: Subtask upload_scores_to_s3
[2024-06-20T11:40:23.218+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:40:23.859+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:40:23.862+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:40:24.499+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:40:25.115+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T11:40:25.116+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:40:25.308+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T114022, end_date=20240620T114025
[2024-06-20T11:40:25.429+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:40:25.564+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:52:24.254+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:52:24.412+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:52:24.440+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:52:24.442+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:52:24.476+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:52:24.527+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '877', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpcbj6jqmx']
[2024-06-20T11:52:24.557+0000] {standard_task_runner.py:91} INFO - Job 877: Subtask upload_scores_to_s3
[2024-06-20T11:52:24.560+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=14965) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:52:24.561+0000] {standard_task_runner.py:63} INFO - Started process 15131 to run task
[2024-06-20T11:52:24.891+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:52:25.542+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:52:25.547+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:52:25.998+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:52:26.551+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T11:52:26.555+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:52:26.685+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T115224, end_date=20240620T115226
[2024-06-20T11:52:26.771+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:52:26.931+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:52:26.940+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:12:20.848+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:12:21.056+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:12:21.102+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:12:21.102+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:12:21.170+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:12:21.224+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=22946) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:12:21.237+0000] {standard_task_runner.py:63} INFO - Started process 23120 to run task
[2024-06-20T12:12:21.227+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '925', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp94_sohfc']
[2024-06-20T12:12:21.242+0000] {standard_task_runner.py:91} INFO - Job 925: Subtask upload_scores_to_s3
[2024-06-20T12:12:21.648+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:12:22.262+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:12:22.267+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:12:22.745+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:12:23.223+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:12:23.230+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:12:23.420+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T121221, end_date=20240620T121223
[2024-06-20T12:12:23.522+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:12:23.599+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:20:45.680+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:20:45.878+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:20:45.894+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:20:45.895+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:20:45.931+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:20:45.984+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=29156) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:20:45.995+0000] {standard_task_runner.py:63} INFO - Started process 29312 to run task
[2024-06-20T12:20:45.990+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '961', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp3oudsxmq']
[2024-06-20T12:20:46.001+0000] {standard_task_runner.py:91} INFO - Job 961: Subtask upload_scores_to_s3
[2024-06-20T12:20:46.465+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:20:47.133+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:20:47.138+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:20:47.674+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:20:48.254+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:20:48.261+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:20:48.415+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T122045, end_date=20240620T122048
[2024-06-20T12:20:48.495+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:20:48.576+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:20:48.580+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:37:46.453+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:37:46.604+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:37:46.633+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:37:46.633+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:37:46.666+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:37:46.707+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=34639) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:37:46.711+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '988', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp30xsnwlo']
[2024-06-20T12:37:46.721+0000] {standard_task_runner.py:91} INFO - Job 988: Subtask upload_scores_to_s3
[2024-06-20T12:37:46.718+0000] {standard_task_runner.py:63} INFO - Started process 34789 to run task
[2024-06-20T12:37:46.979+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:37:47.624+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:37:47.637+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:37:48.354+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:37:48.903+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:37:48.903+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:37:49.123+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T123746, end_date=20240620T123749
[2024-06-20T12:37:49.217+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:37:49.317+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:37:49.324+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:39:39.837+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:39:39.950+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:39:40.012+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:39:40.012+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:39:40.080+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:39:40.127+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=37260) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:39:40.135+0000] {standard_task_runner.py:63} INFO - Started process 37443 to run task
[2024-06-20T12:39:40.130+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1008', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpw3zvfzb5']
[2024-06-20T12:39:40.150+0000] {standard_task_runner.py:91} INFO - Job 1008: Subtask upload_scores_to_s3
[2024-06-20T12:39:40.571+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:39:41.375+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:39:41.376+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:39:42.163+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:39:42.836+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:39:42.840+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:39:42.984+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T123939, end_date=20240620T123942
[2024-06-20T12:39:43.070+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:39:43.145+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:49:13.339+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:49:13.475+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:49:13.522+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:49:13.523+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:49:13.603+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:49:13.655+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=43097) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:49:13.656+0000] {standard_task_runner.py:63} INFO - Started process 43257 to run task
[2024-06-20T12:49:13.653+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1044', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpku4ohikq']
[2024-06-20T12:49:13.659+0000] {standard_task_runner.py:91} INFO - Job 1044: Subtask upload_scores_to_s3
[2024-06-20T12:49:13.863+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:49:14.230+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:49:14.237+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:49:14.967+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:49:15.642+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:49:15.644+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:49:15.840+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T124913, end_date=20240620T124915
[2024-06-20T12:49:15.951+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:49:16.149+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:49:16.152+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:54:43.635+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:54:43.922+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:54:44.017+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:54:44.018+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:54:44.114+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:54:44.187+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=49178) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:54:44.200+0000] {standard_task_runner.py:63} INFO - Started process 49455 to run task
[2024-06-20T12:54:44.189+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1085', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpk2wfufc4']
[2024-06-20T12:54:44.212+0000] {standard_task_runner.py:91} INFO - Job 1085: Subtask upload_scores_to_s3
[2024-06-20T12:54:44.609+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:54:45.153+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:54:45.155+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:54:45.667+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:54:46.054+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T12:54:46.056+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:54:46.194+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T125443, end_date=20240620T125446
[2024-06-20T12:54:46.294+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:54:46.346+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:54:46.349+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T13:06:24.625+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T13:06:24.814+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T13:06:24.869+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T13:06:24.873+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T13:06:24.945+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T13:06:24.996+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=55519) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T13:06:25.002+0000] {standard_task_runner.py:63} INFO - Started process 55685 to run task
[2024-06-20T13:06:24.993+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1118', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp3g4lg984']
[2024-06-20T13:06:25.007+0000] {standard_task_runner.py:91} INFO - Job 1118: Subtask upload_scores_to_s3
[2024-06-20T13:06:25.288+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T13:06:25.896+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T13:06:25.903+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T13:06:26.450+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T13:06:26.949+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T13:06:26.958+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T13:06:27.175+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T130624, end_date=20240620T130627
[2024-06-20T13:06:27.255+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T13:06:27.323+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:34:14.494+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:34:14.635+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:34:14.668+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:34:14.668+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:34:14.750+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:34:14.784+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1153', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpqklcx1h4']
[2024-06-20T16:34:14.795+0000] {standard_task_runner.py:91} INFO - Job 1153: Subtask upload_scores_to_s3
[2024-06-20T16:34:14.803+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4830) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:34:14.803+0000] {standard_task_runner.py:63} INFO - Started process 4993 to run task
[2024-06-20T16:34:15.158+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:34:15.588+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:34:15.589+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:34:16.053+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:34:16.506+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T16:34:16.506+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:34:16.639+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T163414, end_date=20240620T163416
[2024-06-20T16:34:16.741+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:34:16.882+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:34:16.891+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:43:44.007+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:43:44.169+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:43:44.212+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:43:44.212+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:43:44.321+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:43:44.381+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=10504) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:43:44.385+0000] {standard_task_runner.py:63} INFO - Started process 10659 to run task
[2024-06-20T16:43:44.394+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1187', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp5dcynaaq']
[2024-06-20T16:43:44.401+0000] {standard_task_runner.py:91} INFO - Job 1187: Subtask upload_scores_to_s3
[2024-06-20T16:43:44.672+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:43:45.187+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:43:45.188+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:43:45.662+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:43:46.198+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T16:43:46.207+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:43:46.357+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T164344, end_date=20240620T164346
[2024-06-20T16:43:46.424+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:43:46.476+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:54:15.759+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:54:15.851+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:54:15.863+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:54:15.867+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:54:15.886+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:54:15.909+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=15953) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:54:15.911+0000] {standard_task_runner.py:63} INFO - Started process 16120 to run task
[2024-06-20T16:54:15.913+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1215', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpsmd3c9w8']
[2024-06-20T16:54:15.919+0000] {standard_task_runner.py:91} INFO - Job 1215: Subtask upload_scores_to_s3
[2024-06-20T16:54:16.126+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:54:16.381+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:54:16.383+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:54:16.730+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:54:17.280+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T16:54:17.280+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:54:17.419+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T165415, end_date=20240620T165417
[2024-06-20T16:54:17.513+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:54:17.640+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:54:17.643+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:00:35.865+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:00:36.031+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:00:36.066+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:00:36.070+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:00:36.120+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T17:00:36.159+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1247', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpp23l8r0x']
[2024-06-20T17:00:36.176+0000] {standard_task_runner.py:91} INFO - Job 1247: Subtask upload_scores_to_s3
[2024-06-20T17:00:36.170+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=21834) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:00:36.185+0000] {standard_task_runner.py:63} INFO - Started process 21951 to run task
[2024-06-20T17:00:36.443+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:00:36.926+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T17:00:36.927+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:00:37.373+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:00:37.855+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T17:00:37.858+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:00:38.019+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T170036, end_date=20240620T170038
[2024-06-20T17:00:38.109+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:00:38.308+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T17:00:38.318+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:40:53.858+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:40:53.996+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:40:54.021+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:40:54.024+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:40:54.064+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T17:40:54.098+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1281', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpv68rm0t_']
[2024-06-20T17:40:54.107+0000] {standard_task_runner.py:91} INFO - Job 1281: Subtask upload_scores_to_s3
[2024-06-20T17:40:54.102+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=28611) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:40:54.112+0000] {standard_task_runner.py:63} INFO - Started process 28763 to run task
[2024-06-20T17:40:54.400+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:40:54.886+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T17:40:54.887+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:40:55.366+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:40:55.722+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T17:40:55.722+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:40:55.859+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T174053, end_date=20240620T174055
[2024-06-20T17:40:55.917+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:40:56.005+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T17:40:56.015+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:03:18.013+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:03:18.138+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:03:18.177+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:03:18.177+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:03:18.228+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:03:18.258+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1316', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpt2lako6h']
[2024-06-20T18:03:18.270+0000] {standard_task_runner.py:91} INFO - Job 1316: Subtask upload_scores_to_s3
[2024-06-20T18:03:18.262+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=35975) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:03:18.274+0000] {standard_task_runner.py:63} INFO - Started process 36115 to run task
[2024-06-20T18:03:18.562+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:03:19.191+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:03:19.192+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:03:19.644+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:03:20.094+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:03:20.096+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:03:20.210+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T180318, end_date=20240620T180320
[2024-06-20T18:03:20.256+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:03:20.415+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:03:20.424+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:07:25.718+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:07:25.874+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:07:25.910+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:07:25.911+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:07:25.962+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:07:26.007+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=40931) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:07:26.011+0000] {standard_task_runner.py:63} INFO - Started process 41150 to run task
[2024-06-20T18:07:26.004+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1348', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpuml4nbeh']
[2024-06-20T18:07:26.020+0000] {standard_task_runner.py:91} INFO - Job 1348: Subtask upload_scores_to_s3
[2024-06-20T18:07:26.308+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:07:26.736+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:07:26.737+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:07:27.311+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:07:27.858+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:07:27.858+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:07:28.048+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T180725, end_date=20240620T180728
[2024-06-20T18:07:28.096+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:07:28.209+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:07:28.211+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:12:10.535+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:12:10.647+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:12:10.683+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:12:10.684+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:12:10.735+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:12:10.775+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=45970) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:12:10.783+0000] {standard_task_runner.py:63} INFO - Started process 46131 to run task
[2024-06-20T18:12:10.774+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1386', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpc7tn23jr']
[2024-06-20T18:12:10.784+0000] {standard_task_runner.py:91} INFO - Job 1386: Subtask upload_scores_to_s3
[2024-06-20T18:12:11.091+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:12:11.536+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:12:11.537+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:12:11.956+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:12:12.437+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:12:12.438+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:12:12.560+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T181210, end_date=20240620T181212
[2024-06-20T18:12:12.639+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:12:12.788+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:12:12.796+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:16:48.151+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:16:48.297+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:16:48.339+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:16:48.340+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:16:48.416+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:16:48.448+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=50088) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:16:48.454+0000] {standard_task_runner.py:63} INFO - Started process 50266 to run task
[2024-06-20T18:16:48.450+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1417', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpal5xf7gt']
[2024-06-20T18:16:48.456+0000] {standard_task_runner.py:91} INFO - Job 1417: Subtask upload_scores_to_s3
[2024-06-20T18:16:48.710+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:16:49.432+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:16:49.436+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:16:49.855+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:16:50.369+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:16:50.369+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:16:50.452+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T181648, end_date=20240620T181650
[2024-06-20T18:16:50.506+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:16:50.701+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:16:50.708+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:23:53.401+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:23:53.536+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:23:53.565+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:23:53.565+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:23:53.615+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:23:53.648+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1452', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpbmarm00r']
[2024-06-20T18:23:53.667+0000] {standard_task_runner.py:91} INFO - Job 1452: Subtask upload_scores_to_s3
[2024-06-20T18:23:53.652+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=56738) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:23:53.671+0000] {standard_task_runner.py:63} INFO - Started process 56970 to run task
[2024-06-20T18:23:53.984+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:23:54.493+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:23:54.494+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:23:54.860+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:23:55.425+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:23:55.434+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:23:55.557+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T182353, end_date=20240620T182355
[2024-06-20T18:23:55.640+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:23:55.801+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:23:55.809+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:27:19.949+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:27:20.097+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:27:20.124+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:27:20.124+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:27:20.169+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:27:20.199+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=61449) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:27:20.206+0000] {standard_task_runner.py:63} INFO - Started process 61681 to run task
[2024-06-20T18:27:20.201+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1486', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpnzsxfx9w']
[2024-06-20T18:27:20.209+0000] {standard_task_runner.py:91} INFO - Job 1486: Subtask upload_scores_to_s3
[2024-06-20T18:27:20.369+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:27:20.758+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:27:20.759+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:27:21.457+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:27:22.137+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:27:22.149+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:27:22.295+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T182720, end_date=20240620T182722
[2024-06-20T18:27:22.329+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:27:22.504+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:27:22.510+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:30:39.954+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:30:40.228+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:30:40.285+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:30:40.285+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:30:40.333+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:30:40.376+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=65603) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:30:40.387+0000] {standard_task_runner.py:63} INFO - Started process 65792 to run task
[2024-06-20T18:30:40.367+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1522', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp4uptdb69']
[2024-06-20T18:30:40.390+0000] {standard_task_runner.py:91} INFO - Job 1522: Subtask upload_scores_to_s3
[2024-06-20T18:30:40.717+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:30:41.202+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:30:41.203+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:30:41.529+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:30:41.892+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:30:41.892+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:30:41.997+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T183040, end_date=20240620T183041
[2024-06-20T18:30:42.063+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:30:42.159+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:30:42.164+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:37:53.581+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:37:53.744+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:37:53.775+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:37:53.778+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:37:53.818+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:37:53.854+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=71354) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:37:53.865+0000] {standard_task_runner.py:63} INFO - Started process 71535 to run task
[2024-06-20T18:37:53.859+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1556', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpvtdnsf8s']
[2024-06-20T18:37:53.871+0000] {standard_task_runner.py:91} INFO - Job 1556: Subtask upload_scores_to_s3
[2024-06-20T18:37:54.260+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:37:54.762+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:37:54.763+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:37:55.044+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:37:55.493+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T18:37:55.493+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:37:55.731+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T183753, end_date=20240620T183755
[2024-06-20T18:37:55.831+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:37:55.961+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:37:55.967+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:19:32.315+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:19:32.513+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:19:32.551+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:19:32.552+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:19:32.632+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T21:19:32.688+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1592', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxtzmh0sm']
[2024-06-20T21:19:32.702+0000] {standard_task_runner.py:91} INFO - Job 1592: Subtask upload_scores_to_s3
[2024-06-20T21:19:32.703+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3941) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:19:32.714+0000] {standard_task_runner.py:63} INFO - Started process 4100 to run task
[2024-06-20T21:19:33.104+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:19:33.604+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T21:19:33.610+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:19:34.154+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:19:34.677+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T21:19:34.678+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:19:34.776+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T211932, end_date=20240620T211934
[2024-06-20T21:19:34.853+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:19:34.963+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:19:34.966+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:25:00.688+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:25:00.816+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:25:00.858+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:25:00.859+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:25:00.907+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T21:25:00.940+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8847) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:25:00.943+0000] {standard_task_runner.py:63} INFO - Started process 9005 to run task
[2024-06-20T21:25:00.953+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1625', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp6me4my6p']
[2024-06-20T21:25:00.972+0000] {standard_task_runner.py:91} INFO - Job 1625: Subtask upload_scores_to_s3
[2024-06-20T21:25:01.409+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:25:02.071+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T21:25:02.080+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:25:02.691+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:25:03.328+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T21:25:03.333+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:25:03.428+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T212500, end_date=20240620T212503
[2024-06-20T21:25:03.513+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:25:03.671+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:25:03.681+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T22:07:12.552+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T22:07:12.755+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T22:07:12.796+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T22:07:12.797+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T22:07:12.856+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T22:07:12.898+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1680', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprijf0npf']
[2024-06-20T22:07:12.921+0000] {standard_task_runner.py:91} INFO - Job 1680: Subtask upload_scores_to_s3
[2024-06-20T22:07:12.921+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=7976) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T22:07:12.928+0000] {standard_task_runner.py:63} INFO - Started process 8165 to run task
[2024-06-20T22:07:13.308+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T22:07:13.745+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T22:07:13.750+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T22:07:14.107+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T22:07:14.580+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-20T22:07:14.581+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T22:07:14.682+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T220712, end_date=20240620T220714
[2024-06-20T22:07:14.772+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T22:07:14.895+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T22:07:14.897+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
