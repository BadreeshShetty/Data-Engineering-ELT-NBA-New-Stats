[2024-06-19T17:10:31.235+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T17:10:31.287+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:10:31.297+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:10:31.297+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T17:10:31.321+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T17:10:31.349+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '730', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpp_ybumyk']
[2024-06-19T17:10:31.371+0000] {standard_task_runner.py:91} INFO - Job 730: Subtask upload_top_100_stats_to_s3
[2024-06-19T17:10:31.374+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4163) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T17:10:31.374+0000] {standard_task_runner.py:63} INFO - Started process 4205 to run task
[2024-06-19T17:10:31.654+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T17:10:32.066+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T17:10:32.067+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T17:10:32.432+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T17:10:32.840+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023-2024.json
[2024-06-19T17:10:32.840+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T17:10:32.896+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T171031, end_date=20240619T171032
[2024-06-19T17:10:32.930+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T17:10:33.003+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T17:10:33.013+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T17:25:41.230+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T17:25:41.304+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:25:41.324+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:25:41.325+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T17:25:41.353+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T17:25:41.374+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8692) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T17:25:41.371+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '750', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpm1g20ki0']
[2024-06-19T17:25:41.378+0000] {standard_task_runner.py:63} INFO - Started process 8712 to run task
[2024-06-19T17:25:41.378+0000] {standard_task_runner.py:91} INFO - Job 750: Subtask upload_top_100_stats_to_s3
[2024-06-19T17:25:41.531+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T17:25:41.786+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T17:25:41.787+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T17:25:42.006+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T17:25:42.394+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-19T17:25:42.395+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T17:25:42.465+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T172541, end_date=20240619T172542
[2024-06-19T17:25:42.524+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T17:25:42.557+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T21:12:31.959+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T21:12:32.027+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T21:12:32.044+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T21:12:32.045+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T21:12:32.072+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T21:12:32.093+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '772', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpe8c5xira']
[2024-06-19T21:12:32.103+0000] {standard_task_runner.py:91} INFO - Job 772: Subtask upload_top_100_stats_to_s3
[2024-06-19T21:12:32.100+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4567) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T21:12:32.105+0000] {standard_task_runner.py:63} INFO - Started process 4596 to run task
[2024-06-19T21:12:32.241+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T21:12:32.459+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T21:12:32.462+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T21:12:32.718+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T21:12:33.061+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-19T21:12:33.062+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T21:12:33.145+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T211232, end_date=20240619T211233
[2024-06-19T21:12:33.214+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T21:12:33.269+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T21:12:33.271+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:34:12.454+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:34:12.669+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:34:12.736+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:34:12.744+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:34:12.823+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:34:12.898+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '809', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp9eo0yffs']
[2024-06-20T11:34:12.911+0000] {standard_task_runner.py:91} INFO - Job 809: Subtask upload_top_100_stats_to_s3
[2024-06-20T11:34:12.906+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3615) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:34:12.919+0000] {standard_task_runner.py:63} INFO - Started process 3856 to run task
[2024-06-20T11:34:13.362+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:34:13.797+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:34:13.803+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:34:14.269+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:34:14.869+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T11:34:14.883+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:34:15.069+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T113412, end_date=20240620T113415
[2024-06-20T11:34:15.132+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:34:15.188+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:34:15.191+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:40:22.630+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:40:22.777+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:40:22.807+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:40:22.808+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:40:22.852+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:40:22.878+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8043) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:40:22.887+0000] {standard_task_runner.py:63} INFO - Started process 8227 to run task
[2024-06-20T11:40:22.876+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '842', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp28bt_4zg']
[2024-06-20T11:40:22.895+0000] {standard_task_runner.py:91} INFO - Job 842: Subtask upload_top_100_stats_to_s3
[2024-06-20T11:40:23.240+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:40:23.855+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:40:23.857+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:40:24.491+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:40:25.103+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T11:40:25.105+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:40:25.300+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T114022, end_date=20240620T114025
[2024-06-20T11:40:25.425+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:40:25.538+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:52:24.296+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:52:24.406+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:52:24.424+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:52:24.424+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:52:24.453+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:52:24.476+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '879', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpqesbq41l']
[2024-06-20T11:52:24.495+0000] {standard_task_runner.py:91} INFO - Job 879: Subtask upload_top_100_stats_to_s3
[2024-06-20T11:52:24.486+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=14961) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:52:24.501+0000] {standard_task_runner.py:63} INFO - Started process 15127 to run task
[2024-06-20T11:52:24.891+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:52:25.331+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:52:25.334+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:52:25.755+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:52:26.299+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T11:52:26.299+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:52:26.487+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T115224, end_date=20240620T115226
[2024-06-20T11:52:26.586+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:52:26.676+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:52:26.679+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:12:20.797+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:12:20.990+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:12:21.026+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:12:21.027+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:12:21.081+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:12:21.112+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=22947) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:12:21.124+0000] {standard_task_runner.py:63} INFO - Started process 23114 to run task
[2024-06-20T12:12:21.114+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '923', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpljsowm_l']
[2024-06-20T12:12:21.130+0000] {standard_task_runner.py:91} INFO - Job 923: Subtask upload_top_100_stats_to_s3
[2024-06-20T12:12:21.458+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:12:22.123+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:12:22.132+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:12:22.686+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:12:23.254+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T12:12:23.260+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:12:23.427+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T121220, end_date=20240620T121223
[2024-06-20T12:12:23.499+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:12:23.604+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:12:23.611+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:20:45.699+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:20:45.825+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:20:45.854+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:20:45.854+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:20:45.890+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:20:45.928+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=29154) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:20:45.937+0000] {standard_task_runner.py:63} INFO - Started process 29309 to run task
[2024-06-20T12:20:45.940+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '963', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpw0ugoxdm']
[2024-06-20T12:20:45.955+0000] {standard_task_runner.py:91} INFO - Job 963: Subtask upload_top_100_stats_to_s3
[2024-06-20T12:20:46.418+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:20:46.977+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:20:46.978+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:20:47.429+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:20:48.064+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T12:20:48.066+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:20:48.220+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T122045, end_date=20240620T122048
[2024-06-20T12:20:48.299+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:20:48.432+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:20:48.437+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:37:46.318+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:37:46.443+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:37:46.499+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:37:46.501+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:37:46.574+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:37:46.601+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=34640) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:37:46.621+0000] {standard_task_runner.py:63} INFO - Started process 34785 to run task
[2024-06-20T12:37:46.613+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '987', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmppu56mju5']
[2024-06-20T12:37:46.629+0000] {standard_task_runner.py:91} INFO - Job 987: Subtask upload_top_100_stats_to_s3
[2024-06-20T12:37:46.940+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:37:47.558+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:37:47.560+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:37:48.225+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:37:48.977+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T12:37:48.982+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:37:49.179+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T123746, end_date=20240620T123749
[2024-06-20T12:37:49.272+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:37:49.426+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:37:49.432+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:39:40.260+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:39:40.457+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:39:40.478+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:39:40.479+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:39:40.589+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:39:40.646+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=37280) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:39:40.654+0000] {standard_task_runner.py:63} INFO - Started process 37464 to run task
[2024-06-20T12:39:40.648+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1010', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpwlumnxag']
[2024-06-20T12:39:40.672+0000] {standard_task_runner.py:91} INFO - Job 1010: Subtask upload_top_100_stats_to_s3
[2024-06-20T12:39:41.151+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:39:41.800+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:39:41.804+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:39:42.338+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:39:43.081+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T12:39:43.082+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:39:43.305+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T123940, end_date=20240620T123943
[2024-06-20T12:39:43.431+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:39:43.607+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:39:43.612+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:49:14.703+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:49:14.832+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:49:14.864+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:49:14.865+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:49:14.904+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:49:14.941+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=43235) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:49:14.945+0000] {standard_task_runner.py:63} INFO - Started process 43352 to run task
[2024-06-20T12:49:14.939+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1046', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxtpizw0m']
[2024-06-20T12:49:14.949+0000] {standard_task_runner.py:91} INFO - Job 1046: Subtask upload_top_100_stats_to_s3
[2024-06-20T12:49:15.297+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:49:15.884+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:49:15.893+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:49:16.514+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:49:16.912+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T12:49:16.915+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:49:17.052+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T124914, end_date=20240620T124917
[2024-06-20T12:49:17.155+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:49:17.336+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:49:17.344+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:54:42.999+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:54:43.197+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:54:43.273+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:54:43.274+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:54:43.357+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:54:43.414+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1081', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprric8f2q']
[2024-06-20T12:54:43.426+0000] {standard_task_runner.py:91} INFO - Job 1081: Subtask upload_top_100_stats_to_s3
[2024-06-20T12:54:43.422+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=49179) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:54:43.428+0000] {standard_task_runner.py:63} INFO - Started process 49422 to run task
[2024-06-20T12:54:43.891+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:54:44.749+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:54:44.755+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:54:45.387+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:54:47.088+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T12:54:47.091+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:54:47.188+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T125443, end_date=20240620T125447
[2024-06-20T12:54:47.265+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:54:47.382+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:54:47.387+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T13:06:26.536+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T13:06:26.684+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T13:06:26.722+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T13:06:26.722+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T13:06:26.788+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T13:06:26.826+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=55648) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T13:06:26.828+0000] {standard_task_runner.py:63} INFO - Started process 55821 to run task
[2024-06-20T13:06:26.827+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1121', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmppofr_o65']
[2024-06-20T13:06:26.841+0000] {standard_task_runner.py:91} INFO - Job 1121: Subtask upload_top_100_stats_to_s3
[2024-06-20T13:06:27.083+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T13:06:27.531+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T13:06:27.536+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T13:06:28.270+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T13:06:28.876+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T13:06:28.882+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T13:06:29.073+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T130626, end_date=20240620T130629
[2024-06-20T13:06:29.161+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T13:06:29.259+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:34:14.483+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:34:14.625+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:34:14.664+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:34:14.664+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:34:14.724+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:34:14.772+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1154', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprt6zsrqh']
[2024-06-20T16:34:14.787+0000] {standard_task_runner.py:91} INFO - Job 1154: Subtask upload_top_100_stats_to_s3
[2024-06-20T16:34:14.785+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4827) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:34:14.792+0000] {standard_task_runner.py:63} INFO - Started process 4991 to run task
[2024-06-20T16:34:15.041+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:34:15.529+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:34:15.535+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:34:16.055+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:34:16.604+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T16:34:16.610+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:34:16.792+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T163414, end_date=20240620T163416
[2024-06-20T16:34:16.874+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:34:17.072+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:34:17.079+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:43:41.445+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:43:41.599+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:43:41.645+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:43:41.645+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:43:41.695+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:43:41.732+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=10403) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:43:41.742+0000] {standard_task_runner.py:63} INFO - Started process 10515 to run task
[2024-06-20T16:43:41.735+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1185', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp4_2lvx0x']
[2024-06-20T16:43:41.748+0000] {standard_task_runner.py:91} INFO - Job 1185: Subtask upload_top_100_stats_to_s3
[2024-06-20T16:43:41.988+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:43:42.333+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:43:42.334+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:43:42.753+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:43:43.289+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T16:43:43.289+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:43:43.390+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T164341, end_date=20240620T164343
[2024-06-20T16:43:43.423+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:43:43.494+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:43:43.501+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:54:16.247+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:54:16.304+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:54:16.314+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:54:16.314+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:54:16.333+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:54:16.346+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=15955) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:54:16.347+0000] {standard_task_runner.py:63} INFO - Started process 16141 to run task
[2024-06-20T16:54:16.345+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1217', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp_rjntm8e']
[2024-06-20T16:54:16.351+0000] {standard_task_runner.py:91} INFO - Job 1217: Subtask upload_top_100_stats_to_s3
[2024-06-20T16:54:16.531+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:54:16.866+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:54:16.867+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:54:17.463+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:54:18.146+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T16:54:18.148+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:54:18.265+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T165416, end_date=20240620T165418
[2024-06-20T16:54:18.308+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:54:18.407+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:54:18.410+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:00:36.057+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:00:36.249+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:00:36.294+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:00:36.294+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:00:36.354+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T17:00:36.392+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=21842) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:00:36.399+0000] {standard_task_runner.py:63} INFO - Started process 21969 to run task
[2024-06-20T17:00:36.395+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1248', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp577imy13']
[2024-06-20T17:00:36.406+0000] {standard_task_runner.py:91} INFO - Job 1248: Subtask upload_top_100_stats_to_s3
[2024-06-20T17:00:36.730+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:00:36.996+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T17:00:36.997+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:00:37.462+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:00:37.984+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T17:00:37.984+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:00:38.090+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T170036, end_date=20240620T170038
[2024-06-20T17:00:38.157+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:00:38.265+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:40:53.833+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:40:53.994+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:40:54.045+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:40:54.045+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:40:54.092+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T17:40:54.124+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=28609) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:40:54.129+0000] {standard_task_runner.py:63} INFO - Started process 28766 to run task
[2024-06-20T17:40:54.127+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1280', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp_lrufj4u']
[2024-06-20T17:40:54.133+0000] {standard_task_runner.py:91} INFO - Job 1280: Subtask upload_top_100_stats_to_s3
[2024-06-20T17:40:54.458+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:40:54.946+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T17:40:54.947+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:40:55.444+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:40:55.965+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T17:40:55.967+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:40:56.156+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T174053, end_date=20240620T174056
[2024-06-20T17:40:56.212+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:40:56.326+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:03:17.987+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:03:18.162+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:03:18.201+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:03:18.201+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:03:18.278+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:03:18.327+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=35976) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:03:18.328+0000] {standard_task_runner.py:63} INFO - Started process 36119 to run task
[2024-06-20T18:03:18.323+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1313', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpu0dtw_p2']
[2024-06-20T18:03:18.332+0000] {standard_task_runner.py:91} INFO - Job 1313: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:03:18.647+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:03:19.230+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:03:19.231+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:03:19.722+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:03:20.217+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:03:20.217+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:03:20.349+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T180318, end_date=20240620T180320
[2024-06-20T18:03:20.421+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:03:20.539+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:03:20.545+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:07:26.116+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:07:26.293+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:07:26.328+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:07:26.328+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:07:26.383+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:07:26.430+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=40924) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:07:26.441+0000] {standard_task_runner.py:63} INFO - Started process 41160 to run task
[2024-06-20T18:07:26.434+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1349', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp51vlge2h']
[2024-06-20T18:07:26.446+0000] {standard_task_runner.py:91} INFO - Job 1349: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:07:26.722+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:07:27.271+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:07:27.276+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:07:27.694+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:07:28.283+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:07:28.289+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:07:28.429+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T180726, end_date=20240620T180728
[2024-06-20T18:07:28.484+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:07:28.573+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:07:28.576+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:12:10.397+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:12:10.546+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:12:10.597+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:12:10.597+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:12:10.666+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:12:10.708+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=45969) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:12:10.723+0000] {standard_task_runner.py:63} INFO - Started process 46128 to run task
[2024-06-20T18:12:10.714+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1383', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp10fw3uqh']
[2024-06-20T18:12:10.727+0000] {standard_task_runner.py:91} INFO - Job 1383: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:12:11.036+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:12:11.415+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:12:11.416+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:12:11.907+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:12:12.382+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:12:12.382+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:12:12.459+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T181210, end_date=20240620T181212
[2024-06-20T18:12:12.526+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:12:12.644+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:12:12.650+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:16:48.316+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:16:48.436+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:16:48.466+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:16:48.467+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:16:48.498+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:16:48.532+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=50085) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:16:48.533+0000] {standard_task_runner.py:63} INFO - Started process 50269 to run task
[2024-06-20T18:16:48.528+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1418', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpbp8o60ss']
[2024-06-20T18:16:48.537+0000] {standard_task_runner.py:91} INFO - Job 1418: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:16:48.945+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:16:49.409+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:16:49.413+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:16:49.856+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:16:50.471+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:16:50.474+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:16:50.687+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T181648, end_date=20240620T181650
[2024-06-20T18:16:50.785+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:16:50.908+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:16:50.913+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:23:53.531+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:23:53.684+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:23:53.717+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:23:53.718+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:23:53.751+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:23:53.786+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=56746) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:23:53.788+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1453', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpgu8mnv2e']
[2024-06-20T18:23:53.795+0000] {standard_task_runner.py:63} INFO - Started process 56976 to run task
[2024-06-20T18:23:53.798+0000] {standard_task_runner.py:91} INFO - Job 1453: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:23:54.034+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:23:54.492+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:23:54.496+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:23:55.033+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:23:55.614+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:23:55.618+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:23:55.748+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T182353, end_date=20240620T182355
[2024-06-20T18:23:55.883+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:23:55.955+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:27:20.568+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:27:20.725+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:27:20.763+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:27:20.763+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:27:20.809+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:27:20.848+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=61461) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:27:20.862+0000] {standard_task_runner.py:63} INFO - Started process 61702 to run task
[2024-06-20T18:27:20.852+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1487', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp1vtdo5i6']
[2024-06-20T18:27:20.865+0000] {standard_task_runner.py:91} INFO - Job 1487: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:27:21.238+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:27:21.722+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:27:21.723+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:27:22.251+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:27:22.787+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:27:22.787+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:27:22.903+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T182720, end_date=20240620T182722
[2024-06-20T18:27:23.034+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:27:23.122+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:30:39.813+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:30:39.978+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:30:40.017+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:30:40.017+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:30:40.123+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:30:40.163+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1520', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpnxbqeovi']
[2024-06-20T18:30:40.172+0000] {standard_task_runner.py:91} INFO - Job 1520: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:30:40.165+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=65600) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:30:40.175+0000] {standard_task_runner.py:63} INFO - Started process 65780 to run task
[2024-06-20T18:30:40.531+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:30:40.903+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:30:40.906+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:30:41.389+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:30:41.969+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:30:41.972+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:30:42.113+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T183039, end_date=20240620T183042
[2024-06-20T18:30:42.197+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:30:42.315+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:30:42.320+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:37:53.698+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:37:53.844+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:37:53.866+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:37:53.866+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:37:53.922+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:37:53.956+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=71356) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:37:53.962+0000] {standard_task_runner.py:63} INFO - Started process 71540 to run task
[2024-06-20T18:37:53.960+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1557', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp857ayrwo']
[2024-06-20T18:37:53.969+0000] {standard_task_runner.py:91} INFO - Job 1557: Subtask upload_top_100_stats_to_s3
[2024-06-20T18:37:54.338+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:37:54.853+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:37:54.854+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:37:55.472+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:37:56.199+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T18:37:56.206+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:37:56.323+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T183753, end_date=20240620T183756
[2024-06-20T18:37:56.409+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:37:56.573+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:37:56.579+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:19:32.343+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:19:32.559+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:19:32.612+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:19:32.613+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:19:32.764+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T21:19:32.798+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1591', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp0lh50jle']
[2024-06-20T21:19:32.818+0000] {standard_task_runner.py:91} INFO - Job 1591: Subtask upload_top_100_stats_to_s3
[2024-06-20T21:19:32.816+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3939) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:19:32.821+0000] {standard_task_runner.py:63} INFO - Started process 4107 to run task
[2024-06-20T21:19:33.059+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:19:33.644+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T21:19:33.648+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:19:34.156+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:19:34.727+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T21:19:34.727+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:19:34.872+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T211932, end_date=20240620T211934
[2024-06-20T21:19:34.918+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:19:35.019+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:19:35.024+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:25:00.657+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:25:00.825+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:25:00.868+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:25:00.869+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:25:00.920+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T21:25:00.969+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8849) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:25:00.981+0000] {standard_task_runner.py:63} INFO - Started process 9007 to run task
[2024-06-20T21:25:00.975+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1623', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxxuq8mg9']
[2024-06-20T21:25:00.984+0000] {standard_task_runner.py:91} INFO - Job 1623: Subtask upload_top_100_stats_to_s3
[2024-06-20T21:25:01.375+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:25:02.138+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T21:25:02.141+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:25:02.834+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:25:03.441+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T21:25:03.441+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:25:03.643+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T212500, end_date=20240620T212503
[2024-06-20T21:25:03.763+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:25:03.875+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T22:07:12.294+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T22:07:12.486+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T22:07:12.553+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T22:07:12.553+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T22:07:12.634+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_top_100_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T22:07:12.689+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_top_100_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1677', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp61q84_7z']
[2024-06-20T22:07:12.702+0000] {standard_task_runner.py:91} INFO - Job 1677: Subtask upload_top_100_stats_to_s3
[2024-06-20T22:07:12.704+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=7978) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T22:07:12.709+0000] {standard_task_runner.py:63} INFO - Started process 8154 to run task
[2024-06-20T22:07:13.062+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_top_100_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T22:07:13.542+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_top_100_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T22:07:13.546+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T22:07:14.056+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T22:07:14.618+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats_2023_2024.json
[2024-06-20T22:07:14.620+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T22:07:14.762+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_top_100_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T220712, end_date=20240620T220714
[2024-06-20T22:07:14.850+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T22:07:14.937+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T22:07:14.944+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
