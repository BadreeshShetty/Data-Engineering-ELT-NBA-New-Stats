[2024-06-19T00:02:24.805+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T00:02:24.848+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:02:24.857+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:02:24.858+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T00:02:24.871+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T00:02:24.884+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=10001) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T00:02:24.882+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '572', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpuc4_o4fe']
[2024-06-19T00:02:24.885+0000] {standard_task_runner.py:63} INFO - Started process 10024 to run task
[2024-06-19T00:02:24.885+0000] {standard_task_runner.py:91} INFO - Job 572: Subtask upload_stats_to_s3
[2024-06-19T00:02:24.981+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T00:02:25.148+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T00:02:25.149+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T00:02:25.287+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T00:02:25.594+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T00:02:25.595+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T00:02:25.649+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T000224, end_date=20240619T000225
[2024-06-19T00:02:25.666+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T00:02:25.704+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T00:02:25.708+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T00:37:21.877+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T00:37:21.919+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:37:21.928+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:37:21.929+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T00:37:21.941+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T00:37:21.950+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '597', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmppz9zshzm']
[2024-06-19T00:37:21.953+0000] {standard_task_runner.py:91} INFO - Job 597: Subtask upload_stats_to_s3
[2024-06-19T00:37:21.953+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3913) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T00:37:21.954+0000] {standard_task_runner.py:63} INFO - Started process 3921 to run task
[2024-06-19T00:37:22.048+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T00:37:22.205+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T00:37:22.206+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T00:37:22.342+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T00:37:22.535+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T00:37:22.536+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T00:37:22.589+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T003721, end_date=20240619T003722
[2024-06-19T00:37:22.616+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T00:37:22.651+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T00:37:22.653+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T00:47:30.148+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T00:47:30.192+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:47:30.202+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T00:47:30.202+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T00:47:30.216+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T00:47:30.226+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '617', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpmi954vvh']
[2024-06-19T00:47:30.229+0000] {standard_task_runner.py:91} INFO - Job 617: Subtask upload_stats_to_s3
[2024-06-19T00:47:30.229+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=8687) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T00:47:30.230+0000] {standard_task_runner.py:63} INFO - Started process 8695 to run task
[2024-06-19T00:47:30.326+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T00:47:30.482+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T00:47:30.483+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T00:47:30.617+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T00:47:30.915+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T00:47:30.915+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T00:47:30.968+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T004730, end_date=20240619T004730
[2024-06-19T00:47:31.008+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T00:47:31.044+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T00:47:31.046+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T01:10:07.664+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T01:10:07.707+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:10:07.715+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:10:07.716+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T01:10:07.729+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T01:10:07.738+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '637', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpd8am88kh']
[2024-06-19T01:10:07.741+0000] {standard_task_runner.py:91} INFO - Job 637: Subtask upload_stats_to_s3
[2024-06-19T01:10:07.741+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=15192) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T01:10:07.742+0000] {standard_task_runner.py:63} INFO - Started process 15215 to run task
[2024-06-19T01:10:07.840+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T01:10:08.005+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T01:10:08.006+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T01:10:08.166+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T01:10:08.475+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T01:10:08.475+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T01:10:08.529+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T011007, end_date=20240619T011008
[2024-06-19T01:10:08.561+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T01:10:08.599+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T01:10:08.601+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T01:41:03.600+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T01:41:03.646+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:41:03.655+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:41:03.656+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T01:41:03.669+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T01:41:03.680+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '660', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpitg_nb5b']
[2024-06-19T01:41:03.683+0000] {standard_task_runner.py:91} INFO - Job 660: Subtask upload_stats_to_s3
[2024-06-19T01:41:03.687+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=22878) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T01:41:03.690+0000] {standard_task_runner.py:63} INFO - Started process 22886 to run task
[2024-06-19T01:41:03.788+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T01:41:03.948+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T01:41:03.950+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T01:41:04.088+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T01:41:04.324+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T01:41:04.324+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T01:41:04.378+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T014103, end_date=20240619T014104
[2024-06-19T01:41:04.430+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T01:41:04.468+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T01:41:04.470+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T01:56:22.378+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T01:56:22.421+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:56:22.429+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T01:56:22.429+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T01:56:22.442+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T01:56:22.451+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '678', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp4gtlv5ya']
[2024-06-19T01:56:22.454+0000] {standard_task_runner.py:91} INFO - Job 678: Subtask upload_stats_to_s3
[2024-06-19T01:56:22.454+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=27773) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T01:56:22.455+0000] {standard_task_runner.py:63} INFO - Started process 27785 to run task
[2024-06-19T01:56:22.553+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T01:56:22.704+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T01:56:22.705+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T01:56:22.842+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T01:56:23.071+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T01:56:23.072+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T01:56:23.127+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T015622, end_date=20240619T015623
[2024-06-19T01:56:23.153+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T01:56:23.189+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T01:56:23.192+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T04:29:35.789+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T04:29:35.835+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T04:29:35.845+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T04:29:35.846+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T04:29:35.859+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T04:29:35.868+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '695', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpeopepcp6']
[2024-06-19T04:29:35.871+0000] {standard_task_runner.py:91} INFO - Job 695: Subtask upload_stats_to_s3
[2024-06-19T04:29:35.872+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=4180) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T04:29:35.873+0000] {standard_task_runner.py:63} INFO - Started process 4209 to run task
[2024-06-19T04:29:35.967+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T04:29:36.122+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T04:29:36.123+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T04:29:36.272+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T04:29:36.478+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T04:29:36.478+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T04:29:36.532+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T042935, end_date=20240619T042936
[2024-06-19T04:29:36.571+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T04:29:36.608+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T04:29:36.610+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:55:01.014+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:55:01.057+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:55:01.066+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:55:01.066+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T13:55:01.081+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:55:01.093+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '713', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpn6vrcobh']
[2024-06-19T13:55:01.096+0000] {standard_task_runner.py:91} INFO - Job 713: Subtask upload_stats_to_s3
[2024-06-19T13:55:01.096+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=6383) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T13:55:01.097+0000] {standard_task_runner.py:63} INFO - Started process 6406 to run task
[2024-06-19T13:55:01.195+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T13:55:01.353+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:55:01.354+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:55:01.497+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T13:55:01.700+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T13:55:01.701+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:55:01.763+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T135501, end_date=20240619T135501
[2024-06-19T13:55:01.799+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:55:01.837+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:55:01.838+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T17:11:16.690+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T17:11:16.734+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:11:16.744+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:11:16.744+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T17:11:16.758+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T17:11:16.767+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '735', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp7qd7a9gh']
[2024-06-19T17:11:16.771+0000] {standard_task_runner.py:91} INFO - Job 735: Subtask upload_stats_to_s3
[2024-06-19T17:11:16.770+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=5189) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T17:11:16.771+0000] {standard_task_runner.py:63} INFO - Started process 5213 to run task
[2024-06-19T17:11:16.871+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T17:11:17.039+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T17:11:17.040+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T17:11:17.177+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T17:11:17.439+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T17:11:17.439+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T17:11:17.492+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T171116, end_date=20240619T171117
[2024-06-19T17:11:17.550+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T17:11:17.587+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T17:11:17.589+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T17:26:29.417+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T17:26:29.461+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:26:29.470+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T17:26:29.470+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T17:26:29.484+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T17:26:29.494+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=9526) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T17:26:29.495+0000] {standard_task_runner.py:63} INFO - Started process 9534 to run task
[2024-06-19T17:26:29.494+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '756', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpu20a14je']
[2024-06-19T17:26:29.496+0000] {standard_task_runner.py:91} INFO - Job 756: Subtask upload_stats_to_s3
[2024-06-19T17:26:29.591+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T17:26:29.744+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T17:26:29.745+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T17:26:29.886+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T17:26:30.083+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T17:26:30.083+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T17:26:30.137+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T172629, end_date=20240619T172630
[2024-06-19T17:26:30.155+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T17:26:30.193+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T17:26:30.194+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T21:13:18.464+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T21:13:18.507+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T21:13:18.516+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T21:13:18.517+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-19T21:13:18.529+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-19T21:13:18.539+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '777', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpsd53_jek']
[2024-06-19T21:13:18.541+0000] {standard_task_runner.py:91} INFO - Job 777: Subtask upload_stats_to_s3
[2024-06-19T21:13:18.541+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=5420) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-19T21:13:18.542+0000] {standard_task_runner.py:63} INFO - Started process 5428 to run task
[2024-06-19T21:13:18.637+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-19T21:13:18.798+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T21:13:18.799+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T21:13:18.937+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-19T21:13:19.130+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-19T21:13:19.130+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T21:13:19.184+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T211318, end_date=20240619T211319
[2024-06-19T21:13:19.205+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T21:13:19.241+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-19T21:13:19.243+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:35:09.818+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:35:09.865+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:35:09.875+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:35:09.875+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:35:09.889+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:35:09.902+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '818', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxkpemyv3']
[2024-06-20T11:35:09.905+0000] {standard_task_runner.py:91} INFO - Job 818: Subtask upload_stats_to_s3
[2024-06-20T11:35:09.905+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=5135) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:35:09.906+0000] {standard_task_runner.py:63} INFO - Started process 5235 to run task
[2024-06-20T11:35:10.005+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:35:10.170+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:35:10.172+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:35:10.349+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:35:10.635+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T11:35:10.636+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:35:10.697+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T113509, end_date=20240620T113510
[2024-06-20T11:35:10.726+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:35:10.775+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:35:10.777+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:41:01.735+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:41:01.794+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:41:01.810+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:41:01.810+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:41:01.830+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:41:01.851+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=9262) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:41:01.854+0000] {standard_task_runner.py:63} INFO - Started process 9297 to run task
[2024-06-20T11:41:01.850+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '849', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp4549m74m']
[2024-06-20T11:41:01.857+0000] {standard_task_runner.py:91} INFO - Job 849: Subtask upload_stats_to_s3
[2024-06-20T11:41:02.011+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:41:02.174+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:41:02.175+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:41:02.324+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:41:03.077+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T11:41:03.078+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:41:03.143+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T114101, end_date=20240620T114103
[2024-06-20T11:41:03.200+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:41:03.264+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:41:03.269+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T11:53:02.737+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T11:53:02.784+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:53:02.794+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T11:53:02.794+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T11:53:02.809+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T11:53:02.827+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '886', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmptcrn0pt9']
[2024-06-20T11:53:02.832+0000] {standard_task_runner.py:91} INFO - Job 886: Subtask upload_stats_to_s3
[2024-06-20T11:53:02.833+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=16425) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T11:53:02.834+0000] {standard_task_runner.py:63} INFO - Started process 16512 to run task
[2024-06-20T11:53:02.944+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T11:53:03.111+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T11:53:03.112+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T11:53:03.301+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T11:53:04.389+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T11:53:04.390+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T11:53:04.474+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T115302, end_date=20240620T115304
[2024-06-20T11:53:04.503+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T11:53:04.548+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T11:53:04.550+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:13:03.055+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:13:03.122+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:13:03.134+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:13:03.137+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:13:03.154+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:13:03.175+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=24448) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:13:03.179+0000] {standard_task_runner.py:63} INFO - Started process 24485 to run task
[2024-06-20T12:13:03.174+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '934', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp3369w39d']
[2024-06-20T12:13:03.181+0000] {standard_task_runner.py:91} INFO - Job 934: Subtask upload_stats_to_s3
[2024-06-20T12:13:03.339+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:13:03.514+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:13:03.517+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:13:03.690+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:13:04.172+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T12:13:04.172+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:13:04.228+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T121303, end_date=20240620T121304
[2024-06-20T12:13:04.279+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:13:04.324+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:13:04.326+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:21:25.897+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:21:25.990+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:21:26.015+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:21:26.017+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:21:26.044+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:21:26.068+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=30459) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:21:26.071+0000] {standard_task_runner.py:63} INFO - Started process 30527 to run task
[2024-06-20T12:21:26.067+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '970', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp1_kbjv30']
[2024-06-20T12:21:26.073+0000] {standard_task_runner.py:91} INFO - Job 970: Subtask upload_stats_to_s3
[2024-06-20T12:21:26.247+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:21:26.567+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:21:26.570+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:21:26.737+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:21:26.995+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T12:21:26.997+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:21:27.072+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T122125, end_date=20240620T122127
[2024-06-20T12:21:27.097+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:21:27.144+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:21:27.149+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:40:24.576+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:40:24.620+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:40:24.628+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:40:24.629+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:40:24.642+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:40:24.653+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=38806) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:40:24.655+0000] {standard_task_runner.py:63} INFO - Started process 38834 to run task
[2024-06-20T12:40:24.653+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1022', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp6icy_ol2']
[2024-06-20T12:40:24.656+0000] {standard_task_runner.py:91} INFO - Job 1022: Subtask upload_stats_to_s3
[2024-06-20T12:40:24.754+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:40:24.914+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:40:24.916+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:40:25.057+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:40:25.266+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T12:40:25.267+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:40:25.321+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T124024, end_date=20240620T124025
[2024-06-20T12:40:25.352+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:40:25.395+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:40:25.397+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:49:56.775+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:49:56.850+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:49:56.862+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:49:56.862+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:49:56.880+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:49:56.903+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=44601) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:49:56.899+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1057', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp4tc2_02n']
[2024-06-20T12:49:56.906+0000] {standard_task_runner.py:91} INFO - Job 1057: Subtask upload_stats_to_s3
[2024-06-20T12:49:56.905+0000] {standard_task_runner.py:63} INFO - Started process 44669 to run task
[2024-06-20T12:49:57.043+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:49:57.222+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:49:57.227+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:49:57.378+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:49:57.586+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T12:49:57.586+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:49:57.651+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T124956, end_date=20240620T124957
[2024-06-20T12:49:57.694+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:49:57.795+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:49:57.801+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T12:55:24.198+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T12:55:24.268+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:55:24.282+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T12:55:24.283+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T12:55:24.302+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T12:55:24.321+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=50567) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T12:55:24.325+0000] {standard_task_runner.py:63} INFO - Started process 50627 to run task
[2024-06-20T12:55:24.322+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1093', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpykmqtqx3']
[2024-06-20T12:55:24.328+0000] {standard_task_runner.py:91} INFO - Job 1093: Subtask upload_stats_to_s3
[2024-06-20T12:55:24.455+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T12:55:24.624+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T12:55:24.626+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T12:55:24.802+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T12:55:25.075+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T12:55:25.076+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T12:55:25.134+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T125524, end_date=20240620T125525
[2024-06-20T12:55:25.184+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T12:55:25.232+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T12:55:25.236+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T13:07:06.550+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T13:07:06.615+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T13:07:06.632+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T13:07:06.633+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T13:07:06.663+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T13:07:06.693+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=56878) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T13:07:06.695+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1129', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmphmgzrkwo']
[2024-06-20T13:07:06.698+0000] {standard_task_runner.py:63} INFO - Started process 56956 to run task
[2024-06-20T13:07:06.701+0000] {standard_task_runner.py:91} INFO - Job 1129: Subtask upload_stats_to_s3
[2024-06-20T13:07:06.812+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T13:07:06.981+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T13:07:06.984+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T13:07:07.131+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T13:07:07.350+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T13:07:07.351+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T13:07:07.412+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T130706, end_date=20240620T130707
[2024-06-20T13:07:07.446+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T13:07:07.491+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T13:07:07.496+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T16:34:56.330+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T16:34:56.371+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:34:56.379+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T16:34:56.380+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T16:34:56.391+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T16:34:56.405+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=6133) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T16:34:56.407+0000] {standard_task_runner.py:63} INFO - Started process 6149 to run task
[2024-06-20T16:34:56.409+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1164', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprhyb_mp1']
[2024-06-20T16:34:56.412+0000] {standard_task_runner.py:91} INFO - Job 1164: Subtask upload_stats_to_s3
[2024-06-20T16:34:56.528+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T16:34:56.725+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T16:34:56.726+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T16:34:56.853+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T16:34:57.061+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T16:34:57.061+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T16:34:57.110+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T163456, end_date=20240620T163457
[2024-06-20T16:34:57.148+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T16:34:57.187+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T16:34:57.189+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T17:41:35.456+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T17:41:35.499+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:41:35.509+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T17:41:35.510+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T17:41:35.523+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T17:41:35.537+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=29743) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T17:41:35.539+0000] {standard_task_runner.py:63} INFO - Started process 29761 to run task
[2024-06-20T17:41:35.537+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1290', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp9j_o3xni']
[2024-06-20T17:41:35.539+0000] {standard_task_runner.py:91} INFO - Job 1290: Subtask upload_stats_to_s3
[2024-06-20T17:41:35.681+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T17:41:35.831+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T17:41:35.833+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T17:41:35.969+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T17:41:36.241+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T17:41:36.241+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T17:41:36.294+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T174135, end_date=20240620T174136
[2024-06-20T17:41:36.321+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T17:41:36.362+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T17:41:36.364+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:03:59.032+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:03:59.076+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:03:59.084+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:03:59.085+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:03:59.098+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:03:59.114+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=37105) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:03:59.115+0000] {standard_task_runner.py:63} INFO - Started process 37122 to run task
[2024-06-20T18:03:59.113+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1326', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpuedcr773']
[2024-06-20T18:03:59.117+0000] {standard_task_runner.py:91} INFO - Job 1326: Subtask upload_stats_to_s3
[2024-06-20T18:03:59.226+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:03:59.418+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:03:59.419+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:03:59.551+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:03:59.814+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:03:59.814+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:03:59.866+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T180359, end_date=20240620T180359
[2024-06-20T18:03:59.897+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:03:59.938+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:03:59.944+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:08:07.176+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:08:07.217+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:08:07.225+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:08:07.225+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:08:07.237+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:08:07.246+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=42155) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:08:07.247+0000] {standard_task_runner.py:63} INFO - Started process 42167 to run task
[2024-06-20T18:08:07.246+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1359', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp51ogsxke']
[2024-06-20T18:08:07.248+0000] {standard_task_runner.py:91} INFO - Job 1359: Subtask upload_stats_to_s3
[2024-06-20T18:08:07.337+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:08:07.480+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:08:07.481+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:08:07.641+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:08:07.945+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:08:07.945+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:08:08.003+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T180807, end_date=20240620T180808
[2024-06-20T18:08:08.031+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:08:08.071+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:08:08.075+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:12:55.141+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:12:55.181+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:12:55.190+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:12:55.190+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:12:55.205+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:12:55.214+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=46997) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:12:55.215+0000] {standard_task_runner.py:63} INFO - Started process 47098 to run task
[2024-06-20T18:12:55.217+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1395', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpmagqdvfl']
[2024-06-20T18:12:55.219+0000] {standard_task_runner.py:91} INFO - Job 1395: Subtask upload_stats_to_s3
[2024-06-20T18:12:55.312+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:12:55.494+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:12:55.497+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:12:55.631+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:12:55.963+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:12:55.963+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:12:56.014+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T181255, end_date=20240620T181256
[2024-06-20T18:12:56.037+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:12:56.076+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:12:56.077+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:17:28.817+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:17:28.858+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:17:28.867+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:17:28.867+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:17:28.881+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:17:28.892+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=51268) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:17:28.893+0000] {standard_task_runner.py:63} INFO - Started process 51286 to run task
[2024-06-20T18:17:28.894+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1428', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpn3rabekg']
[2024-06-20T18:17:28.896+0000] {standard_task_runner.py:91} INFO - Job 1428: Subtask upload_stats_to_s3
[2024-06-20T18:17:29.035+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:17:29.196+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:17:29.197+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:17:29.326+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:17:29.629+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:17:29.630+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:17:29.679+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T181728, end_date=20240620T181729
[2024-06-20T18:17:29.716+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:17:29.758+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:17:29.759+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:24:34.694+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:24:34.745+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:24:34.753+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:24:34.753+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:24:34.766+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:24:34.776+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=57991) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:24:34.780+0000] {standard_task_runner.py:63} INFO - Started process 58009 to run task
[2024-06-20T18:24:34.781+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1464', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxnot4ar6']
[2024-06-20T18:24:34.786+0000] {standard_task_runner.py:91} INFO - Job 1464: Subtask upload_stats_to_s3
[2024-06-20T18:24:34.930+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:24:35.106+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:24:35.108+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:24:35.243+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:24:35.558+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:24:35.558+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:24:35.609+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T182434, end_date=20240620T182435
[2024-06-20T18:24:35.642+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:24:35.687+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:24:35.690+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:28:01.291+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:28:01.335+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:28:01.344+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:28:01.345+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:28:01.359+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:28:01.376+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=62704) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:28:01.377+0000] {standard_task_runner.py:63} INFO - Started process 62720 to run task
[2024-06-20T18:28:01.375+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1498', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmprvqf836f']
[2024-06-20T18:28:01.379+0000] {standard_task_runner.py:91} INFO - Job 1498: Subtask upload_stats_to_s3
[2024-06-20T18:28:01.503+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:28:01.679+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:28:01.682+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:28:01.821+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:28:02.169+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:28:02.169+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:28:02.220+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T182801, end_date=20240620T182802
[2024-06-20T18:28:02.241+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:28:02.282+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:28:02.284+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:31:20.042+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:31:20.084+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:31:20.094+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:31:20.094+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:31:20.107+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:31:20.116+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=66908) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:31:20.118+0000] {standard_task_runner.py:63} INFO - Started process 66934 to run task
[2024-06-20T18:31:20.117+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1530', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp_mwqcbrt']
[2024-06-20T18:31:20.119+0000] {standard_task_runner.py:91} INFO - Job 1530: Subtask upload_stats_to_s3
[2024-06-20T18:31:20.213+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:31:20.372+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:31:20.373+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:31:20.558+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:31:20.949+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:31:20.949+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:31:21.002+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T183120, end_date=20240620T183121
[2024-06-20T18:31:21.063+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:31:21.104+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:31:21.107+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T18:38:33.059+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T18:38:33.104+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:38:33.113+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T18:38:33.113+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T18:38:33.125+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T18:38:33.141+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=72836) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T18:38:33.144+0000] {standard_task_runner.py:63} INFO - Started process 72871 to run task
[2024-06-20T18:38:33.142+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1567', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpk_p51z_4']
[2024-06-20T18:38:33.145+0000] {standard_task_runner.py:91} INFO - Job 1567: Subtask upload_stats_to_s3
[2024-06-20T18:38:33.292+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T18:38:33.442+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T18:38:33.443+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T18:38:33.579+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T18:38:33.950+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T18:38:33.950+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T18:38:34.009+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T183833, end_date=20240620T183834
[2024-06-20T18:38:34.049+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T18:38:34.090+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T18:38:34.092+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:20:13.761+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:20:13.828+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:20:13.837+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:20:13.837+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:20:13.853+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T21:20:13.880+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=5258) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:20:13.886+0000] {standard_task_runner.py:63} INFO - Started process 5292 to run task
[2024-06-20T21:20:13.877+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1601', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp92kdhxou']
[2024-06-20T21:20:13.887+0000] {standard_task_runner.py:91} INFO - Job 1601: Subtask upload_stats_to_s3
[2024-06-20T21:20:14.055+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:20:14.301+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T21:20:14.302+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:20:14.458+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:20:14.781+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T21:20:14.781+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:20:14.839+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T212013, end_date=20240620T212014
[2024-06-20T21:20:14.870+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:20:14.913+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:20:14.915+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T21:25:45.419+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T21:25:45.465+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:25:45.475+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T21:25:45.475+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T21:25:45.488+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T21:25:45.512+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=10040) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T21:25:45.515+0000] {standard_task_runner.py:63} INFO - Started process 10075 to run task
[2024-06-20T21:25:45.511+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1635', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp9da4bzgs']
[2024-06-20T21:25:45.517+0000] {standard_task_runner.py:91} INFO - Job 1635: Subtask upload_stats_to_s3
[2024-06-20T21:25:45.669+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T21:25:45.827+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T21:25:45.828+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T21:25:45.979+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T21:25:46.202+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T21:25:46.203+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T21:25:46.256+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T212545, end_date=20240620T212546
[2024-06-20T21:25:46.293+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T21:25:46.336+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T21:25:46.339+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-20T22:07:56.991+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-20T22:07:57.035+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T22:07:57.043+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-20T22:07:57.044+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-20T22:07:57.056+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_stats_to_s3> on 2024-06-18 00:00:00+00:00
[2024-06-20T22:07:57.075+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_stats_to_s3', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '1689', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp4zh7w23z']
[2024-06-20T22:07:57.082+0000] {standard_task_runner.py:91} INFO - Job 1689: Subtask upload_stats_to_s3
[2024-06-20T22:07:57.081+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=9450) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-20T22:07:57.084+0000] {standard_task_runner.py:63} INFO - Started process 9465 to run task
[2024-06-20T22:07:57.224+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_stats_to_s3 scheduled__2024-06-18T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-20T22:07:57.392+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_stats_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-20T22:07:57.395+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-20T22:07:57.569+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-20T22:07:57.830+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/player_stats.csv
[2024-06-20T22:07:57.830+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-20T22:07:57.883+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_stats_to_s3, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240620T220757, end_date=20240620T220757
[2024-06-20T22:07:57.903+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-20T22:07:57.945+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-20T22:07:57.947+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
