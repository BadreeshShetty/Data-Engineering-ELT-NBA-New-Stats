[2024-06-18T21:04:26.640+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T21:04:26.772+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:04:26.794+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:04:26.794+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T21:04:26.825+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T21:04:26.860+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '315', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpj9fvwi_f']
[2024-06-18T21:04:26.872+0000] {standard_task_runner.py:91} INFO - Job 315: Subtask upload_scores_to_s3
[2024-06-18T21:04:26.870+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=11002) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T21:04:26.875+0000] {standard_task_runner.py:63} INFO - Started process 11211 to run task
[2024-06-18T21:04:27.129+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T21:04:27.559+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T21:04:27.566+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T21:04:27.913+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T21:04:28.167+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T21:04:28.172+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T21:04:28.299+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T210426, end_date=20240618T210428
[2024-06-18T21:04:28.354+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T21:04:28.430+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T21:04:28.433+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T21:14:49.042+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T21:14:49.089+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:14:49.098+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:14:49.099+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T21:14:49.113+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T21:14:49.143+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=15590) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T21:14:49.145+0000] {standard_task_runner.py:63} INFO - Started process 15622 to run task
[2024-06-18T21:14:49.141+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '334', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp01idwu0n']
[2024-06-18T21:14:49.149+0000] {standard_task_runner.py:91} INFO - Job 334: Subtask upload_scores_to_s3
[2024-06-18T21:14:49.263+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T21:14:49.455+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T21:14:49.458+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T21:14:49.634+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T21:14:49.824+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T21:14:49.825+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T21:14:49.881+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T211449, end_date=20240618T211449
[2024-06-18T21:14:49.933+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T21:14:49.973+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T21:14:49.975+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T21:15:51.104+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T21:15:51.156+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:15:51.165+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:15:51.165+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T21:15:51.180+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T21:15:51.207+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=17607) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T21:15:51.212+0000] {standard_task_runner.py:63} INFO - Started process 17640 to run task
[2024-06-18T21:15:51.213+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '348', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmplmg_v5p5']
[2024-06-18T21:15:51.216+0000] {standard_task_runner.py:91} INFO - Job 348: Subtask upload_scores_to_s3
[2024-06-18T21:15:51.335+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T21:15:51.517+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T21:15:51.520+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T21:15:51.673+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T21:15:51.863+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T21:15:51.863+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T21:15:51.924+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T211551, end_date=20240618T211551
[2024-06-18T21:15:51.959+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T21:15:51.986+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T21:21:12.330+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T21:21:12.457+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:21:12.491+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:21:12.494+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T21:21:12.542+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T21:21:12.590+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=22110) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T21:21:12.594+0000] {standard_task_runner.py:63} INFO - Started process 22212 to run task
[2024-06-18T21:21:12.582+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '382', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpnul7ol_3']
[2024-06-18T21:21:12.606+0000] {standard_task_runner.py:91} INFO - Job 382: Subtask upload_scores_to_s3
[2024-06-18T21:21:12.873+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T21:21:13.187+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T21:21:13.195+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T21:21:13.665+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T21:21:14.084+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T21:21:14.085+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T21:21:14.212+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T212112, end_date=20240618T212114
[2024-06-18T21:21:14.286+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T21:21:14.396+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T21:21:14.401+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T21:46:14.390+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T21:46:14.439+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:46:14.454+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:46:14.455+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T21:46:14.478+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T21:46:14.495+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=28693) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T21:46:14.499+0000] {standard_task_runner.py:63} INFO - Started process 28723 to run task
[2024-06-18T21:46:14.496+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '406', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp6cy6xt1g']
[2024-06-18T21:46:14.501+0000] {standard_task_runner.py:91} INFO - Job 406: Subtask upload_scores_to_s3
[2024-06-18T21:46:14.620+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T21:46:14.801+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T21:46:14.802+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T21:46:14.969+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T21:46:15.164+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T21:46:15.165+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T21:46:15.221+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T214614, end_date=20240618T214615
[2024-06-18T21:46:15.247+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T21:46:15.286+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T21:46:15.288+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T21:48:41.707+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T21:48:41.874+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:48:41.912+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T21:48:41.913+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T21:48:41.970+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T21:48:42.005+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '416', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp3ks8tmf_']
[2024-06-18T21:48:42.020+0000] {standard_task_runner.py:91} INFO - Job 416: Subtask upload_scores_to_s3
[2024-06-18T21:48:42.017+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=30611) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T21:48:42.026+0000] {standard_task_runner.py:63} INFO - Started process 30713 to run task
[2024-06-18T21:48:42.153+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T21:48:42.325+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T21:48:42.326+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T21:48:42.480+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T21:48:42.665+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T21:48:42.666+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T21:48:42.722+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T214841, end_date=20240618T214842
[2024-06-18T21:48:42.771+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T21:48:42.809+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T21:48:42.812+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T22:06:49.029+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T22:06:49.076+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:06:49.086+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:06:49.087+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T22:06:49.104+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T22:06:49.125+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=40228) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T22:06:49.129+0000] {standard_task_runner.py:63} INFO - Started process 40312 to run task
[2024-06-18T22:06:49.121+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '455', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpnlyp9cfq']
[2024-06-18T22:06:49.131+0000] {standard_task_runner.py:91} INFO - Job 455: Subtask upload_scores_to_s3
[2024-06-18T22:06:49.245+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T22:06:49.432+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T22:06:49.435+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T22:06:49.598+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T22:06:49.793+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T22:06:49.793+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T22:06:49.853+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T220649, end_date=20240618T220649
[2024-06-18T22:06:49.879+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T22:06:49.919+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T22:06:49.921+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T22:10:44.121+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T22:10:44.181+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:10:44.191+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:10:44.191+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T22:10:44.210+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T22:10:44.238+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=42937) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T22:10:44.244+0000] {standard_task_runner.py:63} INFO - Started process 42973 to run task
[2024-06-18T22:10:44.248+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '469', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp8gohhhds']
[2024-06-18T22:10:44.258+0000] {standard_task_runner.py:91} INFO - Job 469: Subtask upload_scores_to_s3
[2024-06-18T22:10:44.446+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T22:10:44.629+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T22:10:44.630+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T22:10:44.787+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T22:10:44.988+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T22:10:44.988+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T22:10:45.047+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T221044, end_date=20240618T221045
[2024-06-18T22:10:45.108+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T22:10:45.136+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T22:14:45.364+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T22:14:45.491+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:14:45.534+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:14:45.536+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T22:14:45.667+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T22:14:45.737+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=45818) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T22:14:45.742+0000] {standard_task_runner.py:63} INFO - Started process 45863 to run task
[2024-06-18T22:14:45.720+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '482', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpxnkx3wig']
[2024-06-18T22:14:45.744+0000] {standard_task_runner.py:91} INFO - Job 482: Subtask upload_scores_to_s3
[2024-06-18T22:14:45.910+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T22:14:46.164+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T22:14:46.167+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T22:14:46.487+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T22:14:46.807+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T22:14:46.807+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T22:14:46.919+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T221445, end_date=20240618T221446
[2024-06-18T22:14:46.967+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T22:14:47.034+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T22:14:47.038+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T22:19:26.528+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T22:19:26.579+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:19:26.594+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:19:26.594+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T22:19:26.616+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T22:19:26.649+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=49048) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T22:19:26.650+0000] {standard_task_runner.py:63} INFO - Started process 49095 to run task
[2024-06-18T22:19:26.644+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '498', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp8tf9cocw']
[2024-06-18T22:19:26.653+0000] {standard_task_runner.py:91} INFO - Job 498: Subtask upload_scores_to_s3
[2024-06-18T22:19:26.772+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T22:19:26.947+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T22:19:26.951+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T22:19:27.133+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T22:19:27.324+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T22:19:27.324+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T22:19:27.385+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T221926, end_date=20240618T221927
[2024-06-18T22:19:27.433+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T22:19:27.473+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T22:19:27.475+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T22:34:31.290+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T22:34:31.338+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:34:31.347+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T22:34:31.348+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T22:34:31.362+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T22:34:31.390+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=52476) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T22:34:31.383+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '508', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmp7f339jxu']
[2024-06-18T22:34:31.392+0000] {standard_task_runner.py:91} INFO - Job 508: Subtask upload_scores_to_s3
[2024-06-18T22:34:31.395+0000] {standard_task_runner.py:63} INFO - Started process 52492 to run task
[2024-06-18T22:34:31.535+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T22:34:31.706+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T22:34:31.708+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T22:34:31.869+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T22:34:32.062+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T22:34:32.063+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T22:34:32.129+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T223431, end_date=20240618T223432
[2024-06-18T22:34:32.188+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T22:34:32.241+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T22:34:32.244+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T23:47:39.821+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T23:47:39.867+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T23:47:39.877+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T23:47:39.877+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-06-18T23:47:39.891+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): upload_scores_to_s3> on 2024-06-17 00:00:00+00:00
[2024-06-18T23:47:39.908+0000] {logging_mixin.py:188} WARNING - /home/ubuntu/nba_ns_venv/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=3635) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-18T23:47:39.906+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'nba_data_pipeline_dag', 'upload_scores_to_s3', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '548', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline_dag.py', '--cfg-path', '/tmp/tmpl3480lz3']
[2024-06-18T23:47:39.909+0000] {standard_task_runner.py:91} INFO - Job 548: Subtask upload_scores_to_s3
[2024-06-18T23:47:39.909+0000] {standard_task_runner.py:63} INFO - Started process 3688 to run task
[2024-06-18T23:47:40.050+0000] {task_command.py:426} INFO - Running <TaskInstance: nba_data_pipeline_dag.upload_scores_to_s3 scheduled__2024-06-17T00:00:00+00:00 [running]> on host ip-172-31-45-144.ec2.internal
[2024-06-18T23:47:40.219+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline_dag' AIRFLOW_CTX_TASK_ID='upload_scores_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T23:47:40.221+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T23:47:40.532+0000] {credentials.py:1075} INFO - Found credentials from IAM Role: nba_news_stats_s3
[2024-06-18T23:47:40.761+0000] {python.py:237} INFO - Done. Returned value was: s3://nba-stats-players/nba_scores.json
[2024-06-18T23:47:40.761+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T23:47:40.816+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline_dag, task_id=upload_scores_to_s3, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T234739, end_date=20240618T234740
[2024-06-18T23:47:40.848+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T23:47:40.885+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-18T23:47:40.888+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
